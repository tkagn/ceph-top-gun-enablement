= RadosGW Multisite

:numbered:


== Introduction

A single zone configuration typically consists of one zone group containing one zone and one or more ceph-radosgw instances where you may load-balance gateway client requests between the instances. In a single zone configuration, typically multiple gateway instances point to a single Ceph storage cluster.

image::multisite-intro.png[Multisite Diagram]

Ceph by its own design is synchronous. And so that means from a client's level, write is not acknowledged until all replication copies are written into the cluster. So, for example, if you have a two-way replication cluster with your failure domain at the host level, and your client writes into the cluster, that client does not receive acknowledgment until that second copy has been written to any other node than where the primary write happened. So, you can very quickly see where this would become unusable at the multisite level, especially when the cluster spanned vast geographic distances to one another.

This is where asynchronous replication is used. This allows each cluster in the multisite to become eventually consistent with each other.

So, from the top down, we have realm, Zone Group and zones and I'm going to explain all those right now. So, a realm represents a globally unique namespace and represents the highest level of the Ceph multisite cluster. Each realm can then contain either one or multiple zone groups and multiple zones. So next we have zone groups. These used to be called regions and they're essentially made to define one or more multiple geographic areas. So next we have zones. These are the lowest level of the Ceph multisite configuration and they're represented by one or more object gateways underneath one single Ceph cluster.

== RGW Multisite components

. *Zone*

- Defines a logical group consisting of one or more Ceph Object Gateway instances
- Configuring zones differs from typical Ceph configuration procedures, because not all of the settings end up in a Ceph configuration file
- There will be one zone that should be designated as the Master zone in a zonegroup
- The Master zone will handle all bucket and user creation
- Secondary zone can receive bucket and user operations, but will redirect them to the Master zone
- If the Master zone is down, bucket and user operations will fail
- It is possible to promote a secondary zone to Master zone. This a complex operation. It is recommended only when the Master zone will be down for a long period of time

. *Zone Group*

A zone group is a set of one or more zones. Data stored in one zone in the zone group is
replicated to all other zones in the zone group. One zone in every zone group is designated as
the master zone for that group. The other zones in the zone group are secondary zones.

. *Realm*

A realm represents the global namespace for all objects and buckets in the multisite
replication space. A realm contains one or more zone groups, each of which contains one
or more zones. One zone group in the realm is designated as the master zone group, and
the others are secondary zone groups. All RADOS Gateways in the environment pull their
configuration from the RADOS Gateway in the master zone group and master zone.
Because the master zone in the master zone group handles all metadata updates, operations such
as creating users must occur in the master zone.

. *Multi-zone*

A more advanced configuration consists of one zone group and multiple zones, each zone with one or more ceph-radosgw instances. Each zone is backed by its own Ceph Storage Cluster. Multiple zones in a zone group provides disaster recovery for the zone group should one of the zones experience a significant failure. In Kraken, each zone is active and may receive write operations. In addition to disaster recovery, multiple active zones may also serve as a foundation for content delivery networks.

. *Multi-zone-group*

Formerly called ‘regions’, Ceph Object Gateway can also support multiple zone groups, each zone group with one or more zones. Objects stored to zones in one zone group within the same realm as another zone group will share a global object namespace, ensuring unique object IDs across zone groups and zones.

. *Multiple Realms*

In Kraken, the Ceph Object Gateway supports the notion of realms, which can be a single zone group or multiple zone groups and a globally unique namespace for the realm. Multiple realms provide the ability to support numerous configurations and namespaces.

== Multisite Lab.

In this lab we are going to deploy an advanced configuration that consists
of a single realm with one zone group and two zones, each zone with two Ceph RGW instances. Each
zone is backed by its own Ceph Storage Cluster. Multiple zones in a zone group
provides disaster recovery for the zone group should one of the zones
experience a significant failure. Each zone is active and may receive write
operations.

=== Configure the Master Realm, Zonegroup and Zone.

[TIP]
====
If you have already completed the RadosGW introduction module, you will have
the realm,zonegroup and zone(zone1) created, but with no endpoints configured,
you will need to add valid rgw endpoints to the zonegroup and zone.
====

[WARNING]
====
Run commands on the master/primary cluster.
====

. Create a realm:

+
[source,sh]
----
# radosgw-admin realm create --rgw-realm=multisite --default
{
    "id": "ce31cd75-37c4-4b10-91db-1cda1ca12d95",
    "name": "multisite",
    "current_period": "0ad144e7-a880-43ab-8a64-c9deaf581280",
    "epoch": 1
}
----

. Create a zone group:
+
[source,sh]
----
# radosgw-admin zonegroup create --rgw-zonegroup=multisite_zonegroup --endpoints=http://proxy01:8000 --master --default
{
    "id": "2e41dde9-80f4-4ec8-a099-ec0e8a60938d",
    "name": "multisite_zonegroup",
    "api_name": "multisite_zonegroup",
    "is_master": "true",
    "endpoints": [],
    "hostnames": [],
    "hostnames_s3website": [],
    "master_zone": "",
    "zones": [],
    "placement_targets": [],
    "default_placement": "",
    "realm_id": "ce31cd75-37c4-4b10-91db-1cda1ca12d95",
    "sync_policy": {
        "groups": []
    }
}
----

[NOTE]
====
If you have more than one RGW service running per zone, as you would do for
production, you can add all the rgw address to the endpoints list
--endpoints=http://proxy01:8000,http://ceph-node02:8000 for example, if we want
the sync to survive DNS outages we can use the IP for the endpoints instead
of the Hostnames.
====

. Create a zone:
+
[source,sh]
----
# radosgw-admin zone create --rgw-zonegroup=multisite_zonegroup --rgw-zone=zone1 --access-key=sync --secret=sync --master --default --endpoints=http://proxy01:8000
{
    "id": "0e06b95f-3b6e-4a1c-95e8-b857f699e9e3",
    "name": "zone1",
    "domain_root": "zone1.rgw.meta:root",
    "control_pool": "zone1.rgw.control",
    "gc_pool": "zone1.rgw.log:gc",
    "lc_pool": "zone1.rgw.log:lc",
    "log_pool": "zone1.rgw.log",
    "intent_log_pool": "zone1.rgw.log:intent",
    "usage_log_pool": "zone1.rgw.log:usage",
    "roles_pool": "zone1.rgw.meta:roles",
    "reshard_pool": "zone1.rgw.log:reshard",
    "user_keys_pool": "zone1.rgw.meta:users.keys",
    "user_email_pool": "zone1.rgw.meta:users.email",
    "user_swift_pool": "zone1.rgw.meta:users.swift",
    "user_uid_pool": "zone1.rgw.meta:users.uid",
    "otp_pool": "zone1.rgw.otp",
    "system_key": {
        "access_key": "sync",
        "secret_key": "sync"
    },
    "placement_pools": [
        {
            "key": "default-placement",
            "val": {
                "index_pool": "zone1.rgw.buckets.index",
                "storage_classes": {
                    "STANDARD": {
                        "data_pool": "zone1.rgw.buckets.data"
                    }
                },
                "data_extra_pool": "zone1.rgw.buckets.non-ec",
                "index_type": 0
            }
        }
    ],
    "realm_id": "b3f73708-67c5-4b19-b378-6af9cc66c0b0",
    "notif_pool": "zone1.rgw.log:notif"
}
----

[TIP]
====
We can have one or mode REALMS,ZONEGROUPS or ZONES, if we don't specifiy
them on the radosgw-admin command with --rgw-realm , --rgw-zonegroup= ,
--rgw-zone= , the radosgw-admin command will use the ones set as the defaul
using the --default flag like we did in the previous commands.
====

. Commit the changes:
+
[source,sh]
----
[ceph: root@ceph-mon01 /]# radosgw-admin period update --rgw-realm=multisite --commit
----

. Deploy the RGW daemons with the name `multi.zone1`:
+
[source,sh]
----
[ceph: root@ceph-mon01 /]# ceph orch apply rgw multi.zone1 --realm=multisite --zone=zone1 --placement="2 proxy01 ceph-node02" --port=8000
----
+
[source,texinfo]
----
Scheduled multi.zone1 update...
# ceph orch ps | grep rgw
rgw.multi.zone1.ceph-node02.lviwfb  ceph-node02  *:8000       running (3m)      3m ago   3m    45.7M        -  16.2.8-85.el8cp  b2c997ff1898  0e3521f3a162
rgw.multi.zone1.proxy01.mhawfj      proxy01      *:8000       running (30m)     4m ago  30m    61.9M        -  16.2.8-85.el8cp  b2c997ff1898  4de70934f04e
----

=== Create Sync User

Create a system user that we will use to configure the sync between sites.

----
# radosgw-admin user create --uid=syncuser --display-name="syncuser" --access-key=sync --secret=sync --system
----

=== Configure Seconday Zone

Steps to configure the RADOS Gateway instance on the secondary zone.

[WARNING]
====
Run commands on the seconday Ceph cluster
====

----
# radosgw-admin realm pull --rgw-realm=multisite  --url=http://proxy01:8000 --access-key=sync --secret=sync --default
2022-12-23T09:26:56.377-0500 7fccf8715500  1 error read_lastest_epoch .rgw.root:periods.e7ccb8e8-4a93-4a87-9a6d-8a650696e839.latest_epoch
2022-12-23T09:26:56.415-0500 7fccf8715500  1 Set the period's master zonegroup 6b9fbc87-3202-4a35-85d0-e3e16fc91b32 as the default
{
    "id": "e72107cb-4b3f-49b9-abb0-83c68a9967f9",
    "name": "multisite",
    "current_period": "e7ccb8e8-4a93-4a87-9a6d-8a650696e839",
    "epoch": 2
}
----


Pull the period.
----
# radosgw-admin period pull --url=http://proxy01:8000 --access-key=sync --secret=sync
{
    "id": "e7ccb8e8-4a93-4a87-9a6d-8a650696e839",
    "epoch": 5,
    "predecessor_uuid": "68a74587-6404-4798-83e0-6cd3bf417288",
    "sync_status": [],
    "period_map": {
        "id": "e7ccb8e8-4a93-4a87-9a6d-8a650696e839",
        "zonegroups": [
            {
                "id": "6b9fbc87-3202-4a35-85d0-e3e16fc91b32",
                "name": "multisite_zonegroup",
                "api_name": "multisite_zonegroup",
                "is_master": "true",
                "endpoints": [],
                "hostnames": [],
                "hostnames_s3website": [],
                "master_zone": "c5dc9503-6c11-4851-91bd-f1d5ca61473c",
                "zones": [
                    {
                        "id": "c5dc9503-6c11-4851-91bd-f1d5ca61473c",
                        "name": "zone1",
                        "endpoints": [],
                        "log_meta": "false",
                        "log_data": "false",
                        "bucket_index_max_shards": 11,
                        "read_only": "false",
                        "tier_type": "",
                        "sync_from_all": "true",
                        "sync_from": [],
                        "redirect_zone": ""
                    }
                ],
                "placement_targets": [
                    {
                        "name": "default-placement",
                        "tags": [],
                        "storage_classes": [
                            "SSD",
                            "STANDARD"
                        ]
                    },
                    {
                        "name": "ssd",
                        "tags": [
                            "allowed-ssd"
                        ],
                        "storage_classes": [
                            "STANDARD"
                        ]
                    }
                ],
                "default_placement": "default-placement",
                "realm_id": "e72107cb-4b3f-49b9-abb0-83c68a9967f9",
                "sync_policy": {
                    "groups": []
                }
            }
        ],
        "short_zone_ids": [
            {
                "key": "c5dc9503-6c11-4851-91bd-f1d5ca61473c",
                "val": 2695141038
            }
        ]
    },
    "master_zonegroup": "6b9fbc87-3202-4a35-85d0-e3e16fc91b32",
    "master_zone": "c5dc9503-6c11-4851-91bd-f1d5ca61473c",
    "period_config": {
        "bucket_quota": {
            "enabled": false,
            "check_on_raw": false,
            "max_size": -1,
            "max_size_kb": 0,
            "max_objects": -1
        },
        "user_quota": {
            "enabled": false,
            "check_on_raw": false,
            "max_size": -1,
            "max_size_kb": 0,
            "max_objects": -1
        }
    },
    "realm_id": "e72107cb-4b3f-49b9-abb0-83c68a9967f9",
    "realm_name": "multisite",
    "realm_epoch": 2
}
----

Create a secondary zone.

----
# radosgw-admin zone create --rgw-zone=zone2 --rgw-zonegroup=multisite_zonegroup --endpoints=http://proxy02:8000 --access-key=sync --secret=sync --default
2022-12-23T09:28:04.140-0500 7f905d907500  0 failed reading obj info from .rgw.root:zone_info.c5dc9503-6c11-4851-91bd-f1d5ca61473c: (2) No such file or directory
2022-12-23T09:28:04.140-0500 7f905d907500  0 WARNING: could not read zone params for zone id=c5dc9503-6c11-4851-91bd-f1d5ca61473c name=zone1
{
    "id": "5c14f28b-72f2-4323-aa35-24bd1cb8fc0e",
    "name": "zone2",
    "domain_root": "zone2.rgw.meta:root",
    "control_pool": "zone2.rgw.control",
    "gc_pool": "zone2.rgw.log:gc",
    "lc_pool": "zone2.rgw.log:lc",
    "log_pool": "zone2.rgw.log",
    "intent_log_pool": "zone2.rgw.log:intent",
    "usage_log_pool": "zone2.rgw.log:usage",
    "roles_pool": "zone2.rgw.meta:roles",
    "reshard_pool": "zone2.rgw.log:reshard",
    "user_keys_pool": "zone2.rgw.meta:users.keys",
    "user_email_pool": "zone2.rgw.meta:users.email",
    "user_swift_pool": "zone2.rgw.meta:users.swift",
    "user_uid_pool": "zone2.rgw.meta:users.uid",
    "otp_pool": "zone2.rgw.otp",
    "system_key": {
        "access_key": "sync",
        "secret_key": "sync"
    },
    "placement_pools": [
        {
            "key": "default-placement",
            "val": {
                "index_pool": "zone2.rgw.buckets.index",
                "storage_classes": {
                    "STANDARD": {
                        "data_pool": "zone2.rgw.buckets.data"
                    }
                },
                "data_extra_pool": "zone2.rgw.buckets.non-ec",
                "index_type": 0
            }
        }
    ],
    "realm_id": "e72107cb-4b3f-49b9-abb0-83c68a9967f9",
    "notif_pool": "zone2.rgw.log:notif"
}
----

Commit the changes.

----
# radosgw-admin period update --commit
Sending period to new master zone c5dc9503-6c11-4851-91bd-f1d5ca61473c
{
    "id": "e7ccb8e8-4a93-4a87-9a6d-8a650696e839",
    "epoch": 7,
    "predecessor_uuid": "68a74587-6404-4798-83e0-6cd3bf417288",
    "sync_status": [],
    "period_map": {
        "id": "e7ccb8e8-4a93-4a87-9a6d-8a650696e839",
        "zonegroups": [
            {
                "id": "6b9fbc87-3202-4a35-85d0-e3e16fc91b32",
                "name": "multisite_zonegroup",
                "api_name": "multisite_zonegroup",
                "is_master": "true",
                "endpoints": [
                    "http://proxy01:8000"
                ],
                "hostnames": [],
                "hostnames_s3website": [],
                "master_zone": "c5dc9503-6c11-4851-91bd-f1d5ca61473c",
                "zones": [
                    {
                        "id": "c5dc9503-6c11-4851-91bd-f1d5ca61473c",
                        "name": "zone1",
                        "endpoints": [
                            "http://proxy01:8000"
                        ],
                        "log_meta": "false",
                        "log_data": "true",
                        "bucket_index_max_shards": 11,
                        "read_only": "false",
                        "tier_type": "",
                        "sync_from_all": "true",
                        "sync_from": [],
                        "redirect_zone": ""
                    },
                    {
                        "id": "ec5a7187-95e1-4bf2-8519-208175c81487",
                        "name": "zone2",
                        "endpoints": [
                            "http://proxy02:8000"
                        ],
                        "log_meta": "false",
                        "log_data": "true",
                        "bucket_index_max_shards": 11,
                        "read_only": "false",
                        "tier_type": "",
                        "sync_from_all": "true",
                        "sync_from": [],
                        "redirect_zone": ""
                    }
                ],
                "placement_targets": [
                    {
                        "name": "default-placement",
                        "tags": [],
                        "storage_classes": [
                            "SSD",
                            "STANDARD"
                        ]
                    },
                    {
                        "name": "ssd",
                        "tags": [
                            "allowed-ssd"
                        ],
                        "storage_classes": [
                            "STANDARD"
                        ]
                    }
                ],
                "default_placement": "default-placement",
                "realm_id": "e72107cb-4b3f-49b9-abb0-83c68a9967f9",
                "sync_policy": {
                    "groups": []
                }
            }
        ],
        "short_zone_ids": [
            {
                "key": "c5dc9503-6c11-4851-91bd-f1d5ca61473c",
                "val": 2695141038
            },
            {
                "key": "ec5a7187-95e1-4bf2-8519-208175c81487",
                "val": 3374434257
            }
        ]
    },
    "master_zonegroup": "6b9fbc87-3202-4a35-85d0-e3e16fc91b32",
    "master_zone": "c5dc9503-6c11-4851-91bd-f1d5ca61473c",
    "period_config": {
        "bucket_quota": {
            "enabled": false,
            "check_on_raw": false,
            "max_size": -1,
            "max_size_kb": 0,
            "max_objects": -1
        },
        "user_quota": {
            "enabled": false,
            "check_on_raw": false,
            "max_size": -1,
            "max_size_kb": 0,
            "max_objects": -1
        }
    },
    "realm_id": "e72107cb-4b3f-49b9-abb0-83c68a9967f9",
    "realm_name": "multisite",
    "realm_epoch": 2
}
----

Create the RADOS Gateway service for the secondary zone.

----
# ceph orch apply rgw multi.zone2 --realm=multisite --zone=zone2 --placement="2 proxy02 ceph-mon02" --port=8000
----

Use the radosgw-admin sync status command, we can see the sync is started and a
full copy of the master zone is being synced with the secondary zone

----
# radosgw-admin sync status
          realm e72107cb-4b3f-49b9-abb0-83c68a9967f9 (multisite)
      zonegroup 6b9fbc87-3202-4a35-85d0-e3e16fc91b32 (multisite_zonegroup)
           zone ec5a7187-95e1-4bf2-8519-208175c81487 (zone2)
   current time 2022-12-23T14:41:08Z
  metadata sync syncing
                full sync: 1/64 shards
                full sync: 21 entries to sync
                incremental sync: 63/64 shards
                metadata is behind on 1 shards
                behind shards: [0]
      data sync source: c5dc9503-6c11-4851-91bd-f1d5ca61473c (zone1)
                        syncing
                        full sync: 63/128 shards
                        full sync: 77 buckets to sync
                        incremental sync: 65/128 shards
                        data is behind on 63 shards
                        behind shards: [4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,36,37,38,39,40,41,42,43,44,45,46,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,105,106,107,108,109,110,111,112,113,114,115,116]
----

[TIP]
====
The output can differ depending on the sync status. The shards are described as two different types during sync:
- Behind shards are shards that need a full data sync and shards needing an incremental data sync because they are not up-to-date.
- Recovery shards are shards that encountered an error during sync and marked for retry. The error mostly occurs on minor issues like acquiring a lock on a bucket. This will typically resolve itself.
====

[NOTE]
====
If you encounter sync errors in your configuration, with shards falling behind
, you can run the commandi `# radosgw-admin  sync error list`.
Also increasing the verbosity of
the RGW logs is a good place to start looking for errors, to increase the
verbosity you can follow the steps of this
https://access.redhat.com/solutions/2085183[KCS]
====

After a while if we run the same command we will probably see metadata and data in sync:

----
# radosgw-admin sync status
          realm 4818713d-4bdf-4ef7-ab7b-c9ceb8009bdb (multisite)
      zonegroup ce0533e9-ebe7-45f4-8126-91e9f9253599 (multisite_zonegroup)
           zone d0492b20-abca-463a-8972-9eae824537fd (zone2)
   current time 2022-12-24T10:52:29Z
  metadata sync syncing
                full sync: 0/64 shards
                incremental sync: 64/64 shards
                metadata is caught up with master
      data sync source: 4913e13d-17a9-4c6f-96a4-91b87d2cfe68 (zone1)
                        syncing
                        full sync: 0/128 shards
                        incremental sync: 128/128 shards
                        data is caught up with source
----

With this current configuration every data object will be synced
bi-directionally on both sites, so we can upload objects to site1 or
site2(Active/Active) and they 
we will get replicated in async mode between sites, using the terme eventually
consistent.

[WARNING]
====
Remember that metadata changes should only be done on the master node,
the master node will take care of replicating the metadata changes to the rest
of the zones in the zonegroup
====

[TIP]
====
By default, the objects are not verified again after the synchronization of an object was successful. To enable that, you can set rgw_sync_obj_etag_verify to true. After enabling the optional objects that will be synchronized going forward, an additional MD5 checksum will verify that it is computed on the source and the destination. This is to ensure the integrity of the objects fetched from a remote server over HTTP including multisite sync. This option can decrease the performance of your RGW as more computation is needed.
====

We can see the sync direction configuration using `radosgw-admin sync info`
command, we can see that sources and destinations are replicating `*` all
buckets and their data between sites.

----
# radosgw-admin sync info
{
    "sources": [
        {
            "id": "all",
            "source": {
                "zone": "zone1",
                "bucket": "*"
            },
            "dest": {
                "zone": "zone2",
                "bucket": "*"
            },
            "params": {
                "source": {
                    "filter": {
                        "tags": []
                    }
                },
                "dest": {},
                "priority": 0,
                "mode": "system",
                "user": ""
            }
        }
    ],
    "dests": [
        {
            "id": "all",
            "source": {
                "zone": "zone2",
                "bucket": "*"
            },
            "dest": {
                "zone": "zone1",
                "bucket": "*"
            },
            "params": {
                "source": {
                    "filter": {
                        "tags": []
                    }
                },
                "dest": {},
                "priority": 0,
                "mode": "system",
                "user": ""
            }
        }
    ],
----

[TIP]
====
For multi-site only, you can check out the metadata log (mdlog), the bucket index log (bilog) and the data log (datalog). You can list them and also trim them which is not needed in most cases as rgw_sync_log_trim_interval is set to 20 minutes as default. You shouldn’t have to trim it at any time as it could cause side effects otherwise.
====

Let's check if metadata and data replication is working
fine, all metadata changes have to be in the primary site, so I'm going to
create a user, and we can check how it's synced to the secondary site.


----
[ceph-node01 ~]# radosgw-admin user create --uid=multiuser --display-name="multiuser" --access-key=multiuser --secret=multiuser --system

[root@ceph-mon01 ~]# radosgw-admin user list
[
    "syncuser",
    "dashboard",
    "multiuser"
]
----

Using the multiuser user we just created let's upload some objects from each
site and check that they are getting replicated in both directions

[NOTE]
====
We change the endpoint in the AWS CLI when we want to interact with the primary
or secondary cluster, proxy01 is zone1, and proxy02 is zone2.
====


----
# aws --endpoint http://proxy01:8000 s3 mb s3://bucket1
# aws --endpoint http://proxy01:8000 s3 ls
2022-12-29 03:59:14 bucket1
# aws --endpoint http://proxy02:8000 s3 ls
2022-12-29 03:59:14 bucket1
# aws --endpoint  http://proxy02:8000 s3 ls s3://bucket1/
2022-12-29 04:18:01       1330 file1
# aws --endpoint  http://proxy02:8000 s3 cp /etc/hosts s3://bucket1/file2
# aws --endpoint  http://proxy01:8000 s3 ls s3://bucket1/
2022-12-29 04:18:01       1330 file1
2022-12-29 04:18:54       1330 file2
----

We have now confirmed that metadata and data sync replication is working fine
in our deployment.

There is a new feature in RGW multisite called Sync policies that gives greater
flexibility on how we sync our data.

Multisite bucket-granularity sync policy provides fine grained control of data movement between buckets in different zones. It extends the zone sync mechanism. Previously buckets were being treated symmetrically, that is – each (data) zone holds a mirror of that bucket that should be the same as all the other zones. Whereas leveraging the bucket-granularity sync policy is possible for buckets to diverge, and a bucket can pull data from other buckets (ones that don’t share its name or its ID) in different zone. The sync process was assuming therefore that the bucket sync source and the bucket sync destination were always referring to the same bucket, now that is not the case anymore.

The sync policy supersedes the old zonegroup coarse configuration (sync_from*). The sync policy can be configured at the zonegroup level (and if it is configured it replaces the old style config), but it can also be configured at the bucket level.

In the sync policy multiple groups that can contain lists of data-flow configurations can be defined, as well as lists of pipe configurations. The data-flow defines the flow of data between the different zones. It can define symmetrical data flow, in which multiple zones sync data from each other, and it can define directional data flow, in which the data moves in one way from one zone to another.

This new feature opens up many configuration options for our multisite
replication, in this lab we are just going to show one example, were we
configure bucket replication granularity, and only configure replication
between sites for bucket1. you can check more examples in the upstream https://docs.ceph.com/en/quincy/radosgw/multisite-sync-policy/#examples[DOC]

We configure the zonegroup sync policy group that needs to be in place(flow +
pipe) to be able to configure bucket sync policy

[NOTE]
====
Any changes to the zonegroup policy needs to be applied on the zonegroup master zone, and require period update and commit.
====

----
+----------------------------+----------------------------------------+
|  Value                     | Description                            |
+============================+========================================+
| ``enabled``                | sync is allowed and enabled            |
+----------------------------+----------------------------------------+
| ``allowed``                | sync is allowed                        |
+----------------------------+----------------------------------------+
| ``forbidden``              | sync (as defined by this group) is not |
|                            | allowed and can override other groups  |
+----------------------------+----------------------------------------+
----

We create the zonegroup sync group and set the replication status to allowed

----
# radosgw-admin sync group create --group-id=group1 --status=allowed
{
    "groups": [
        {
            "id": "group1",
            "data_flow": {},
            "pipes": [],
            "status": "allowed"
        }
    ]
}
----

Now we create a flow for the group, setting the flow bi-derectional/symmetrical
for zones: zone1,zone2

----
# radosgw-admin sync group flow create --group-id=group1  --flow-id=flow-symmetrical --flow-type=symmetrical --zones=zone1,zone2
{
    "groups": [
        {
            "id": "group1",
            "data_flow": {
                "symmetrical": [
                    {
                        "id": "flow-symmetrical",
                        "zones": [
                            "zone2",
                            "zone1"
                        ]
                    }
                ]
            },
            "pipes": [],
            "status": "allowed"
        }
    ]
}
----

Finally we set on the zonegroup sync policy a pipe were we allow replication of
all buckets from all zones in the group

----
radosgw-admin sync group pipe create --group-id=group1 --pipe-id=pipe1 --source-zones='*' --source-bucket='*' --dest-zones='*' --dest-bucket='*'
{
    "groups": [
        {
            "id": "group1",
            "data_flow": {
                "symmetrical": [
                    {
                        "id": "flow-symmetrical",
                        "zones": [
                            "zone2",
                            "zone1"
                        ]
                    }
                ]
            },
            "pipes": [
                {
                    "id": "pipe1",
                    "source": {
                        "bucket": "*",
                        "zones": [
                            "*"
                        ]
                    },
                    "dest": {
                        "bucket": "*",
                        "zones": [
                            "*"
                        ]
                    },
                    "params": {
                        "source": {
                            "filter": {
                                "tags": []
                            }
                        },
                        "dest": {},
                        "priority": 0,
                        "mode": "system",
                        "user": ""
                    }
                }
            ],
            "status": "allowed"
        }
    ]
}
----

We need to do a period update so the changes we made to the zonegroup
replication are reflected on both sites

----
# radosgw-admin period update --commit
----

But remember that the status of the replication for the zonegroup is set to
`allowed` not `enabled` so there is currently no replication between sites, we
can confirm it with the s3 cli.

----
[root@ceph-node01 ~]# aws --endpoint  http://proxy01:8000 s3 cp /etc/hosts s3://bucket1/file11
upload: ../etc/hosts to s3://bucket1/file11                    
[root@ceph-node01 ~]# aws --endpoint  http://proxy01:8000 s3 ls s3://bucket1/
2022-12-29 04:18:01       1330 file1
2022-12-29 04:38:20       1330 file11
2022-12-29 04:18:54       1330 file2
[root@ceph-node01 ~]# aws --endpoint  http://proxy02:8000 s3 ls s3://bucket1/
2022-12-29 04:18:01       1330 file1
2022-12-29 04:18:54       1330 file2
----

Now that the zonegroup is in place and is `allowing` replication, we can
configure the sync policy at the bucket level, we only want to enable sync for
bucket `bucket1` , we create a new sync group and pipe for `bucket1`


----
# radosgw-admin sync group create --bucket=bucket1 --group-id=bucket1-default --status=enabled
{
    "groups": [
        {
            "id": "bucket1-default",
            "data_flow": {},
            "pipes": [],
            "status": "enabled"
        }
    ]
}

# radosgw-admin sync group pipe create --bucket=bucket1 --group-id=bucket1-default --pipe-id=pipe1 --source-zones='*' --dest-zones='*'
# radosgw-admin sync group get --bucket bucket1
[
    {
        "key": "bucket1-default",
        "val": {
            "id": "bucket1-default",
            "data_flow": {},
            "pipes": [
                {
                    "id": "pipe1",
                    "source": {
                        "bucket": "*",
                        "zones": [
                            "*"
                        ]
                    },
                    "dest": {
                        "bucket": "*",
                        "zones": [
                            "*"
                        ]
                    },
                    "params": {
                        "source": {
                            "filter": {
                                "tags": []
                            }
                        },
                        "dest": {},
                        "priority": 0,
                        "mode": "system",
                        "user": "multiuser"
                    }
                }
            ],
            "status": "enabled"
        }
    }
]
----

We can check the replication policies configured for a bucket with the help of
the `radosgw-admin sync info` command, because the flow we configured for
replication is bi-directional, we can see that bucket replication for bucket1
is configured as source and destination

----
# radosgw-admin sync info --bucket bucket1
{
    "sources": [
        {
            "id": "pipe1",
            "source": {
                "zone": "zone2",
                "bucket": "bucket1:a315bff5-2e58-495b-b297-95383c1e0ab3.24584.1"
            },
            "dest": {
                "zone": "zone1",
                "bucket": "bucket1:a315bff5-2e58-495b-b297-95383c1e0ab3.24584.1"
            },
            "params": {
                "source": {
                    "filter": {
                        "tags": []
                    }
                },
                "dest": {},
                "priority": 0,
                "mode": "system",
                "user": "multiuser"
            }
        }
    ],
    "dests": [
        {
            "id": "pipe1",
            "source": {
                "zone": "zone1",
                "bucket": "bucket1:a315bff5-2e58-495b-b297-95383c1e0ab3.24584.1"
            },
            "dest": {
                "zone": "zone2",
                "bucket": "bucket1:a315bff5-2e58-495b-b297-95383c1e0ab3.24584.1"
            },
            "params": {
                "source": {
                    "filter": {
                        "tags": []
                    }
                },
                "dest": {},
                "priority": 0,
                "mode": "system",
                "user": "multiuser"
            }
        }
    ],
    "hints": {
        "sources": [],
        "dests": []
    },
    "resolved-hints-1": {
        "sources": [],
        "dests": []
    },
    "resolved-hints": {
        "sources": [],
        "dests": []
    }
}
----

Bucket policy modifications don't need a period update, they are automaticly
detected, so let's go ahead and test if bucket replication is working for
bucket1: 

----
# aws --endpoint  http://proxy01:8000 s3 cp /etc/hosts s3://bucket1/file111
upload: ../etc/hosts to s3://bucket1/file111
# aws --endpoint  http://proxy02:8000 s3 ls s3://bucket1/
2022-12-29 04:18:01       1330 file1
2022-12-29 04:42:34       1330 file111
2022-12-29 04:18:54       1330 file2
----

[TIP]
====
You could achieve the same bucket granular replication by using the
`radosgw-admin bucket sync [enable/disable] --bucket=<bucket>` command, but
take into account the multisite sync policies are so much more powerfull and
flexible
====

Just to double check we can create a new bucket called `bucket2` and see how
this bucket is not getting replicated as there is no policy in place to enable
this replication.

----
# aws --endpoint http://proxy01:8000 s3 mb s3://bucket2
# aws --endpoint  http://proxy01:8000 s3 cp /etc/hosts s3://bucket2/file1
upload: ../etc/hosts to s3://bucket2/file1
# aws --endpoint  http://proxy02:8000 s3 ls s3://bucket2/
#
----

If we want to also enable sync in this bucket we can go ahead with

----
# radosgw-admin sync group create --bucket=bucket2 --group-id=bucket2-default --status=enabled
# radosgw-admin sync group pipe create --bucket=bucket2 --group-id=bucket2-default --pipe-id=pipe1 --source-zones='*' --dest-zones='*'
# aws --endpoint  http://proxy01:8000 s3 cp /etc/hosts s3://bucket2/file2
upload: ../etc/hosts to s3://bucket2/file2
# aws --endpoint  http://proxy02:8000 s3 ls s3://bucket2/
2022-12-29 05:10:03       1330 file2
----


