<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Ceph Architecture :: Ceph Top Gun Enablement</title>
    <link rel="canonical" href="https://likid0.github.io/ceph-top-gun-enablement/training/ceph/ceph_architecture.html">
    <meta name="generator" content="Antora 3.0.1">
    <link rel="stylesheet" href="../../_/css/site.css">
    <script>var uiRootPath = '../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://docs.ceph.com/en/latest/" target="_blank">
          <img src="../../_/img/header_logo_reverse.svg" height="48px" alt="Ceph">
      </a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Get Help</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://access.redhat.com/documentation/zh-cn/red_hat_ceph_storage/5" target="_blank">Ceph Documentation</a>
            <a class="navbar-item" href="https://bugzilla.redhat.com/describecomponents.cgi?product=Red%20Hat%20OpenShift%20Container%20Storage" target="_blank">Browse Bugs</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Improve Guides</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://github.com/likid0/ceph-top-gun-enablement/issues/new/choose" target="_blank">Open Issue</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">More Infos</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://www.redhat.com/en/blog/channel/red-hat-storage" target="_blank">Our Blog</a>
            <a class="navbar-item" href="https://www.youtube.com/channel/UCoyG8VyvB-XUxQl1mD3T3Gw" target="_blank">Youtube</a>
            <a class="navbar-item" href="https://docs.ceph.com/en/latest/" target="_blank">Ceph Storage Technology</a>
          </div>
        </div>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="training" data-version="master">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Ceph Top-Gun Enablement</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Lab Setup</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="opentlc_lab_env.html">Opentlc Lab Env</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Core Ceph</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_introduction.html">Ceph Introduction</a>
  </li>
  <li class="nav-item is-current-page" data-depth="1">
    <a class="nav-link" href="ceph_architecture.html">Ceph Architecture</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cluster_partitioning.html">Ceph Cluster Partitioning</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_hardware.html">Ceph Hardware Recommendations</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_deployment_intro.html">Ceph Install Methods </a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cephadm_intro.html">Cephadm Orchestrator</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_deploy_basic.html">Deploy Ceph with Cephadm</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cli_intro.html">Ceph CLI basic commands</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_pools.html">Ceph storage pools config</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_bluestore.html">Ceph OSD Bluestore</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_recovery.html">Ceph OSD Failure/Recovery</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph-upgrades_cephadm.html">Upgrade Ceph with Cephadm</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_deployment_challenge.html">Challenge Ceph Deployment</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph RADOS Block Device</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephrbd_intro.html">RADOS Block Device introduction</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">CephFS Shared FileSystem</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephfs_intro.html">CephFS introduction &amp; Deployment</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephfs_advanced.html">CephFS Deep Dive</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cephfs_challenge.html">Challenge Cephfs</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph RadosGW</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_intro.html">RGW Introduction &amp; Deployment</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_arch_deep_dive.html">RGW Deep Dive</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_ha.html">RGW High Availability</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_ssl.html">RGW &amp; Ingress with SSL</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_users_quotas.html">RGW Users &amp; Quotas</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_auth.html">RGW Auth &amp; Authz</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_object_versioning.html">RGW S3 Object Versioning</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_placement_and_storage_classes.html">RGW Placement &amp; Storage Classes</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_life_cycle_management.html">RGW Life Cycle Management</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_bucket_policy.html">RGW S3 Bucket Policy</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_sts_introduction.html">RGW Secure Token Service</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_sts_bucket_role_policy.html">RGW Bucket vs Role Policy</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_multisite.html">RGW Multisite Replication</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_cloudsync.html">RGW Object Cloud Transition</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_presignedurl.html">RGW presigned URL</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_opslog.html">RGW Opslog</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_bucket_notification.html">RGW bucket Notification</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_rgw_challenge.html">Challenge RGW</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Troubleshooting</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="trouble-shooting-nearfull-osds.html">Troubleshooting nearfull OSDs</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="troubleshooting_bluestore.html">Troubleshooting Bluestore issues</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="trouble-shooting-large-omap-objects.html">Troubleshooting Large Omap Objects</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Benchmarking</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_performance_example.html">Setting the Inital Baseline</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Stretched</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="rhcs-stretched-deploy.html">Ceph Stretch Mode</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Ceph Top-Gun Enablement</span>
    <span class="version">master</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">Ceph Top-Gun Enablement</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">master</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Ceph Top-Gun Enablement</a></li>
    <li>Core Ceph</li>
    <li><a href="ceph_architecture.html">Ceph Architecture</a></li>
  </ul>
</nav>
  <div class="edit-this-page"><a href="file:///antora/training/modules/ceph/pages/ceph_architecture.adoc">Edit this Page</a></div>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Ceph Architecture</h1>
<div id="toc" class="toc">
<div id="toctitle">Table of Contents</div>
<ul class="sectlevel1">
<li><a href="#_ceph_architecture">1. Ceph architecture</a>
<ul class="sectlevel2">
<li><a href="#_rados_and_its_components">1.1. RADOS and its components</a></li>
<li><a href="#_rados_access_methods">1.2. RADOS access methods</a></li>
<li><a href="#_controlled_replication_under_scalable_hashing">1.3. Controlled replication under scalable hashing</a></li>
</ul>
</li>
</ul>
</div>
<div class="sect1">
<h2 id="_ceph_architecture"><a class="anchor" href="#_ceph_architecture"></a>1. Ceph architecture</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The Ceph cluster provides a scalable storage solution while providing
multiple access methods to enable the different types of
clients present within the IT infrastructure to get access to the data.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="_images/ceph101-overview.png" alt="Ceph From Above">
</div>
<div class="title">Figure 1. Ceph Architecture</div>
</div>
<div class="paragraph">
<p>The entire Ceph architecture is resilient, does not present any single point
of failure (SPOF) and introduces a specific data placement algorithm known as CRUSH (Controlled
Replication Under Scalable Hashing) that does not require the use of a central look-up server.</p>
</div>
<div class="sect2">
<h3 id="_rados_and_its_components"><a class="anchor" href="#_rados_and_its_components"></a>1.1. RADOS and its components</h3>
<div class="paragraph">
<p>The heart of Ceph is an object store known as RADOS (Reliable Autonomic
Distributed Object Store) bottom layer on the screen. This layer provides the
Ceph software defined storage with the ability to store data (serve IO
requests, to protect the data, to check the consistency and the integrity of
the data through built-in mechanisms. The RADOS layer is composed of the
following daemons:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>MONs or Monitors</p>
</li>
<li>
<p>OSDs or Object Storage Daemons</p>
</li>
<li>
<p>MGRs or Managers</p>
</li>
<li>
<p>MDSs or Meta Data Servers</p>
</li>
</ul>
</div>
<div class="paragraph">
<div class="title"><strong><em>Monitors</em></strong></div>
<p>The Monitors maintain the cluster map and state and provide distributed
decision-making while configured in an odd number, 3 or 5 depending on the
size and the topology of the cluster, to prevent split-brain situations. The
Monitors are not in the data-path and do not serve IO requests to and from
the clients.</p>
</div>
<div class="paragraph">
<p>The Monitors use the Paxos algorithm to maintain map consistency. It requires
a majority (50% + 1) of the Monitors to validate each map update event. This is why the Monitors
must be deployed as an odd number. All Monitors maintain an identical copy of the
cluster map which contains the following maps:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>MONMap</code> - Map of the Monitors</p>
</li>
<li>
<p><code>MGRMap</code> - Map of the Managers</p>
</li>
<li>
<p><code>MDSMap</code> - Map of the MDSs</p>
</li>
<li>
<p><code>OSDMap</code> - Map of the OSDs</p>
</li>
<li>
<p><code>PGMap</code>  - Map of the Placement Groups</p>
</li>
<li>
<p><code>SVCMap</code> - Map of the services offered by the Ceph cluster</p>
</li>
</ul>
</div>
<div class="paragraph">
<div class="title"><strong><em>OSDs</em></strong></div>
<p>One Object Storage Daemon is typically deployed for each local block devices and the native
scalable nature of Ceph allows for thousands of OSDs to be part of the
cluster. The OSDs are serving IO requests from the clients while guaranteeing
the protection of the data (replication or erasure coding), the rebalancing
of the data in case of an OSD or a node failure, the coherence of the data
(scrubbing and deep-scrubbing of the existing data).</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="_images/ceph101-osdlayout.png" alt="Layout">
</div>
<div class="title">Figure 2. OSD Layout</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Originally the OSD backend was using a filesystem and a transaction journal to
guarantee the atomicity of the write transactions. This implementation, known
as the FileStore, has been sunset and replaced by a new backend architecture
based on RocksDB and known as BlueStore (<code>Jewel</code> introduction).
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>The OSDs can have two (2) roles assigned to them. The role depends on what an OSD
performs as functions for a given Placement Group. Therefor a given OSD can be assigned
the two (2) roles at the same time but for different Placement Groups (<code>PGs</code>).</p>
</div>
<div class="paragraph">
<p><strong>Primary OSD</strong> for a specific <code>PG</code>:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Serves <code>PG</code> IO requests</p>
</li>
<li>
<p>Replicates and protects <code>PG</code></p>
</li>
<li>
<p>Checks <code>PG</code> coherence</p>
</li>
<li>
<p>Rebalances <code>PG</code></p>
</li>
<li>
<p>Recovers <code>PG</code></p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Secondary OSD</strong> for a specific <code>PG</code>:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Always controlled by the Primary OSD for the <code>PG</code></p>
</li>
<li>
<p>Can be promoted Primary OSD</p>
</li>
</ul>
</div>
<div class="paragraph">
<div class="title"><strong><em>MGRs</em></strong></div>
<p>The Managers are tightly integrated with the Monitors and collect the
statistics within the cluster. Additionally they provide an extensible
framework for the cluster through a pluggable Python interface aimed at
expanding the Ceph existing capabilities. The current list of modules
developed around the Manager framework are:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Balancer module</p>
</li>
<li>
<p>Placement Group auto-scaler module</p>
</li>
<li>
<p>Dashboard module</p>
</li>
<li>
<p>RESTful module</p>
</li>
<li>
<p>Prometheus module</p>
</li>
<li>
<p>Zabbix module</p>
</li>
<li>
<p>Rook module</p>
</li>
</ul>
</div>
<div class="paragraph">
<div class="title"><strong><em>MDSs</em></strong></div>
<p>The Meta Data Servers manage the metadata for the POSIX compliant shared
filesystem such as the directory hierarchy and the file metadata (ownership,
timestamps, mode, &#8230;&#8203;). All the metadata is stored with RADOS and they do not
server any data to the clients. MDSs are only deployed when a shared
filesystem is configured in the Ceph cluster.</p>
</div>
<div class="paragraph">
<div class="title"><strong><em>General Layout</em></strong></div>
<p>If we look at the Ceph cluster foundation layer, the full picture with the
different types of daemons or containers looks like this.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="_images/ceph101-rados.png" alt="RADOS Overview">
</div>
<div class="title">Figure 3. RADOS as it stands</div>
</div>
<div class="paragraph">
<p>The circle represent the MONs, the 'M' represent the MGRs and the squares
with the bars represent the OSDs. In the diagram above, the cluster operates
with 3 Monitors, 2 Managers and 23 OSDs.</p>
</div>
</div>
<div class="sect2">
<h3 id="_rados_access_methods"><a class="anchor" href="#_rados_access_methods"></a>1.2. RADOS access methods</h3>
<div class="paragraph">
<p>Ceph was designed to provides the IT environment with all the necessary
access methods so that any application can use what is the best solution for
its use-case.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="_images/ceph101-differentstoragetypes.png" alt="Ceph Access Modes">
</div>
<div class="title">Figure 4. Different Storage Types Supported</div>
</div>
<div class="paragraph">
<p>Ceph supports block storage through the RADOS Block Device (aka RBD) access
method, file storage through the Ceph Filesystem (aka CephFS) access method
and object storage through its native <code>librados</code> API or through the RADOS
Gateway (aka RADOSGW or RGW) for compatibility with the S3 and Swift
protocols.</p>
</div>
<div class="paragraph">
<p><strong>Librados</strong></p>
</div>
<div class="paragraph">
<p>Librados allows developers to code natively against the native Ceph cluster
API for maximum efficiency combined with a small footprint.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="_images/ceph101-librados.png" alt="librados">
</div>
<div class="title">Figure 5. Application Native Object API</div>
</div>
<div class="paragraph">
<p>The Ceph native API offers different wrappers such as C, C++, Python, Java,
Ruby, Erlang, Go and Rust.</p>
</div>
<div class="paragraph">
<p><strong>RADOS Block Device (RBD)</strong></p>
</div>
<div class="paragraph">
<p>This access method is used with Linux distributions and Kubernetes.
RBDs can be accessed either through a kernel module (Linux, Kubernetes)
or through the <code>librbd</code> API (OpenStack, Proxmox). In the Kubernetes world,
RBDs are designed to address the need for RWO PVCs. <code>Pacific</code> provides a
major update to <code>librbd</code> aimed at reducing the performance gap with <code>kRBD</code>.</p>
</div>
<div class="paragraph">
<p><strong><em>Kernel Module (kRBD)</em></strong></p>
</div>
<div class="paragraph">
<p>The kernel RBD driver offers superior performance while not providing the
same level of functionality. e.g., no RBD Mirroring until <code>Pacific</code>.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="_images/ceph101-krbd.png" alt="Kernel based RADOS Block Device">
</div>
<div class="title">Figure 6. kRBD Diagram</div>
</div>
<div class="paragraph">
<p><strong><em>Userspace RBD (librbd)</em></strong></p>
</div>
<div class="paragraph">
<p>This access method leverages all existing RBD features such as RBD Mirroring.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="_images/ceph101-librbd.png" alt="Userspace RADOS Block Device">
</div>
<div class="title">Figure 7. librbd Diagram</div>
</div>
<div class="paragraph">
<p><strong><em>Shared filesystem (CephFS)</em></strong></p>
</div>
<div class="paragraph">
<p>This method allows clients to jointly access a shared POSIX compliant
filesystem. The client initially contacts the Meta Data Server to obtain the
location of the object(s) for a given inode and then communicates directly
with an OSD to perform the final IO request.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="_images/ceph101-cephfs.png" alt="Kernel Based CephFS Client">
</div>
<div class="title">Figure 8. File Access (Ceph Filesystem or CephFS)</div>
</div>
<div class="paragraph">
<p>In Kubernetes environments, CephFS is typically used for RWX claims and can
also be used to support RWO claims.</p>
</div>
<div class="paragraph">
<p><strong><em>Object storage, S3 and Swift (Ceph RADOS Gateway)</em></strong></p>
</div>
<div class="paragraph">
<p>This access method offers support for the Amazon S3 and OpenStack Swift
support on top of a Ceph cluster.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="_images/ceph101-rgw.png" alt="S3 and Swift Support">
</div>
<div class="title">Figure 9. Amazon S3 or OpenStack Swift (Ceph RADOS Gateway)</div>
</div>
</div>
<div class="sect2">
<h3 id="_controlled_replication_under_scalable_hashing"><a class="anchor" href="#_controlled_replication_under_scalable_hashing"></a>1.3. Controlled replication under scalable hashing</h3>
<div class="paragraph">
<p>The Ceph cluster being a distributed architecture some solution had to be
designed to provide an efficient way to distribute the data across the
multiple OSDs in the cluster. The technique used is called CRUSH or
Controlled Replication Under Scalable Hashing. With CRUSH, every object is
assigned to one and only one hash bucket known as a Placement Group (PG).</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="_images/ceph101-crushfromobjecttoosd-new.png" alt="From Object to OSD">
</div>
</div>
<div class="paragraph">
<p>CRUSH is the central point of configuration for the topology of the cluster.
It offers a pseudo-random placement algorithm to distribute the objects
across the PGs and uses rules to determine the mapping of the PGs to the
OSDs. In essence, the PGs are an abstraction layer between the objects
(application layer) and the OSDs (physical layer). In case of failure, the
PGs will be remapped to different physical devices (OSDs) and eventually see
their content resynchronized to match the protection rules selected by the
storage administrator.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The <code>PG</code> acts as a floating abstraction layer between the object (logical layer)
and the OSD (physical layer). The mapping of a <code>PG</code> is linked to the actual cluster
state (Cluster Map). As a result the mapping of a <code>PG</code> will always be
identical for a given cluster state.
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <a class="navbar-item" href="https://www.redhat.com/en/technologies/cloud-computing/openshift-container-storage" target="_blank">
      <img src="../../_/img/header_logo.svg" alt="Ceph">
  </a>
</footer>
<script id="site-script" src="../../_/js/site.js" data-ui-root-path="../../_"></script>
<script async src="../../_/js/vendor/highlight.js"></script>
  </body>
</html>
