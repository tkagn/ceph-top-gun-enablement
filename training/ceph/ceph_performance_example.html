<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Setting the Ceph Cluster Baseline before we start Performance Testing :: Ceph Top Gun Enablement</title>
    <link rel="canonical" href="https://likid0.github.io/ceph-top-gun-enablement/training/ceph/ceph_performance_example.html">
    <meta name="generator" content="Antora 3.0.1">
    <link rel="stylesheet" href="../../_/css/site.css">
    <script>var uiRootPath = '../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://docs.ceph.com/en/latest/" target="_blank">
          <img src="../../_/img/header_logo_reverse.svg" height="48px" alt="Ceph">
      </a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Get Help</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://access.redhat.com/documentation/zh-cn/red_hat_ceph_storage/5" target="_blank">Ceph Documentation</a>
            <a class="navbar-item" href="https://bugzilla.redhat.com/describecomponents.cgi?product=Red%20Hat%20OpenShift%20Container%20Storage" target="_blank">Browse Bugs</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Improve Guides</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://github.com/likid0/ceph-top-gun-enablement/issues/new/choose" target="_blank">Open Issue</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">More Infos</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://www.redhat.com/en/blog/channel/red-hat-storage" target="_blank">Our Blog</a>
            <a class="navbar-item" href="https://www.youtube.com/channel/UCoyG8VyvB-XUxQl1mD3T3Gw" target="_blank">Youtube</a>
            <a class="navbar-item" href="https://docs.ceph.com/en/latest/" target="_blank">Ceph Storage Technology</a>
          </div>
        </div>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="training" data-version="master">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Ceph Top-Gun Enablement</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Lab Setup</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="opentlc_lab_env.html">Opentlc Lab Env</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Core Ceph</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_introduction.html">Ceph Introduction</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_architecture.html">Ceph Architecture</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cluster_partitioning.html">Ceph Cluster Partitioning</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_hardware.html">Ceph Hardware Recommendations</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_deployment_intro.html">Ceph Install Methods </a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cephadm_intro.html">Cephadm Orchestrator</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_deploy_basic.html">Deploy Ceph with Cephadm</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cli_intro.html">Ceph CLI basic commands</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_pools.html">Ceph storage pools config</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_bluestore.html">Ceph OSD Bluestore</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_recovery.html">Ceph OSD Failure/Recovery</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph-upgrades_cephadm.html">Upgrade Ceph with Cephadm</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph RADOS Block Device</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephrbd_intro.html">RADOS Block Device introduction</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">CephFS Shared FileSystem</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephfs_intro.html">CephFS introduction &amp; Deployment</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephfs_advanced.html">CephFS Deep Dive</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph RadosGW</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_intro.html">RGW Introduction &amp; Deployment</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_arch_deep_dive.html">RGW Deep Dive</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_ha.html">RGW High Availability</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_ssl.html">RGW &amp; Ingress with SSL</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_users_quotas.html">RGW Users &amp; Quotas</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_auth.html">RGW Auth &amp; Authz</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_object_versioning.html">RGW S3 Object Versioning</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_placement_and_storage_classes.html">RGW Placement &amp; Storage Classes</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_life_cycle_management.html">RGW Life Cycle Management</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_bucket_policy.html">RGW S3 Bucket Policy</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_sts_introduction.html">RGW Secure Token Service</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_sts_bucket_role_policy.html">RGW Bucket vs Role Policy</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_multisite.html">RGW Multisite Replication</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_cloudsync.html">RGW Object Cloud Transition</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_presignedurl.html">RGW presigned URL</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_opslog.html">RGW Opslog</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_bucket_notification.html">RGW bucket Notification</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Troubleshooting</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="trouble-shooting-nearfull-osds.html">Troubleshooting nearfull OSDs</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="troubleshooting_bluestore.html">Troubleshooting Bluestore issues</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="trouble-shooting-large-omap-objects.html">Troubleshooting Large Omap Objects</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Benchmarking</span>
<ul class="nav-list">
  <li class="nav-item is-current-page" data-depth="1">
    <a class="nav-link" href="ceph_performance_example.html">Setting the Inital Baseline</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Stretched</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="rhcs-stretched-deploy.html">Ceph Stretch Mode</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Ceph Top-Gun Enablement</span>
    <span class="version">master</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">Ceph Top-Gun Enablement</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">master</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Ceph Top-Gun Enablement</a></li>
    <li>Ceph Benchmarking</li>
    <li><a href="ceph_performance_example.html">Setting the Inital Baseline</a></li>
  </ul>
</nav>
  <div class="edit-this-page"><a href="file:///antora/training/modules/ceph/pages/ceph_performance_example.adoc">Edit this Page</a></div>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Setting the Ceph Cluster Baseline before we start Performance Testing</h1>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>When deploying a brand new ceph cluster, and before starting the benchmarking
or taking the cluster into production, it&#8217;s always a good idea to do some basic
baseline testing to be sure all the server components are working as expected.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_define_software_versions"><a class="anchor" href="#_define_software_versions"></a>1. Define Software Versions</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Let&#8217;s layout the versions of the software that we will be using during the
testing</p>
</div>
<div class="paragraph">
<p><strong>Example:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>OS.</strong> RHEL 8.7(Kernel )</p>
</li>
<li>
<p><strong>Ceph.</strong> RHCS 5.3</p>
</li>
<li>
<p><strong>Ansible.</strong> 2.9</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_define_hardware"><a class="anchor" href="#_define_hardware"></a>2. Define Hardware</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_ceph_nodes"><a class="anchor" href="#_ceph_nodes"></a>2.1. Ceph Nodes</h3>
<div class="paragraph">
<p>Let&#8217;s layout the the hardware that we will be used during the
testing</p>
</div>
<div class="paragraph">
<p><strong>Example:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Manufacturer and model: Cisco UCS C220-M5</p>
</li>
<li>
<p>Processor: 2x 2.5 GHz 8180/205W 28C/38.50MB Cache/DDR4 2666MHz</p>
</li>
<li>
<p>Memory: 192 GB</p>
<div class="ulist">
<ul>
<li>
<p>Disk Layout:</p>
<div class="ulist">
<ul>
<li>
<p>2 x 220 GB SAS/SSD RAID1 for the Operating System.</p>
</li>
<li>
<p>7 x 4 TB P4500 NVMe disks to use as Ceph OSDs.</p>
</li>
<li>
<p>1 x 375 GB Intel Optane P4800X disk to store WAL/RocksDB Bluestore components.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Network Interface Cards: The High Availability is managed at the Cisco Fabric Interconnect layer, so only 2 nics are presented to the OS for Ceph:
***1 x 40 Gbps NIC for the Ceph public network.</p>
</li>
<li>
<p>1 x 40 Gbps NIC for the Ceph cluster network.</p>
</li>
<li>
<p>Power Management: IPMI functionality enabled on the server’s motherboard.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_client_nodes"><a class="anchor" href="#_client_nodes"></a>2.2. Client Nodes</h3>
<div class="paragraph">
<p>Not only the Ceph server specification is importan, the client resources are a
fundamental part of the testing as we will need enough firepower from the
client side to saturate our ceph cluster</p>
</div>
<div class="paragraph">
<p><strong>Example:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Manufacturer and model: UCS B200 M4 Blade servers</p>
</li>
<li>
<p>Processor: 2x Intel&#174; Xeon&#174; CPU E5-2640 v4 @ 2.40GHz</p>
</li>
<li>
<p>Memory: 528 GB</p>
</li>
<li>
<p>Network Interface Cards:</p>
<div class="ulist">
<ul>
<li>
<p>1 x 20 Gbps NICs for the OSP instances overlay uplink.</p>
</li>
<li>
<p>1 x 20 Gbps NICs for the underlay OSP networks</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_update_hardware_firmwaredrivers_to_latest"><a class="anchor" href="#_update_hardware_firmwaredrivers_to_latest"></a>3. Update Hardware Firmware/Drivers to latest.</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Update Firmware and Drivers for all the Hardware components involved.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_configure_bios_options_for_performance"><a class="anchor" href="#_configure_bios_options_for_performance"></a>4. Configure BIOS options for performance.</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This is dependent on the Hardware being used, but in general we recommend to disable P-states and C-states on the Intel® processors, and also to ensure consistency of CPU performance, we disabled Enhanced Intel SpeedStep® technology (EIST) and Intel® Turbo Boost Technology.</p>
</div>
<div class="paragraph">
<p><strong>Example:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">BIOS Options
Enhanced Speedstep Disabled
Turbo Boost Disabled
Autonomous Core C-Sate Disabled
Processor C State Disabled
Processor C1E Disabled
Processor C3 Report Disabled
Processor C6 Report Disabled
Processor C7 Report Disabled
Global C-State  Control Disabled</code></pre>
</div>
</div>
<div class="paragraph">
<p>Very related to the specific Hardware/NiCs that are being used, in many
ocassions no need to modify. Cisco UCS for example does need modifications so
it&#8217;s always good to check</p>
</div>
<div class="paragraph">
<p>Number of Transmit and receive queue for each NIC configured on the system, The ring buffer size.</p>
</div>
<div class="paragraph">
<p>* Once the ring buffer has been increased, allowing more traffic to be stored to achieve higher traffic rates.
* Increasing the number of queues allows multiple CPU cores to work to service the ring buffer at the same time.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The values used here are just an example for a specific workload, don&#8217;t
copy and paste into other configurations
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p><strong>Example:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">Ethernet Adapter policy Options
Transmit Queues 8
Tx Ring Size 4096
Receive Queues 8
Rx Ring Size 4096
Completion Queues 16
Receive Side Scaling (RSS) Enabled
Accelerated Receive Flow Steering Enabled</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_configure_kernel_parameters"><a class="anchor" href="#_configure_kernel_parameters"></a>5. Configure Kernel Parameters</h2>
<div class="sectionbody">
<div class="paragraph">
<p>A custom Tuned profile for the OS can be used if needed, depending on the Workload</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The values used here are just an example for a specific workload, don&#8217;t
copy and paste into other configurations
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p><strong>Example:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">[main]
summary=ceph_perf

[cpu]
governor=performance
energy_perf_bias=performance
min_perf_pct=100
force_latency=1

[disk]
readahead=&gt;4096

[sysctl]
kernel.sched_min_granularity_ns = 10000000
kernel.sched_wakeup_granularity_ns = 15000000
fs.aio-max-nr=1048576
kernel.pid_max=4194303
fs.file-max=26234859
vm.zone_reclaim_mode=0
vm.swappiness=1
vm.dirty_ratio = 10
vm.dirty_background_ratio = 5
net.ipv4.tcp_rmem=4096 87380 134217728
net.ipv4.tcp_wmem=4096 65536 134217728
net.core.rmem_max=268435456
net.core.wmem_max=268435456
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_fin_timeout = 10
net.ipv4.tcp_slow_start_after_idle = 0
net.ipv4.conf.all.send_redirects = 0
net.ipv4.conf.all.accept_redirects = 0
net.ipv4.conf.all.accept_source_route = 0
net.ipv4.tcp_mtu_probing = 1
net.ipv4.tcp_timestamps = 0
net.core.netdev_max_backlog = 50000
net.ipv4.tcp_max_syn_backlog = 30000
net.ipv4.tcp_max_tw_buckets = 2000000</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_pre_test_verifications"><a class="anchor" href="#_pre_test_verifications"></a>6. Pre-Test Verifications</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_burn_in_tools"><a class="anchor" href="#_burn_in_tools"></a>6.1. Burn-in Tools</h3>
<div class="paragraph">
<p>Burn-in tests are usually performed on hardware to enable the detection of any problems before the performance tests of Ceph begin, the tests will be run individually on each component. If any component is defective, it is most likely to be detected during the Burn-in test, so the defective part can be replaced before the performance tests are run in this way it won’t affect the results in any way.</p>
</div>
<div class="paragraph">
<p>There are many Burn-in tools, just as an example we cxan use stress-ng tool. stress-ng will stress test a computer system in various selectable ways. It was designed to exercise various physical subsystems of a computer as well as the various operating system kernel interfaces.</p>
</div>
<div class="paragraph">
<p><strong>Example:</strong></p>
</div>
<div class="paragraph">
<p>The test will be run on the Ceph OSD servers concurrently during 168 hours, the command that will be run, will exercise CPU, memory and the hard disks.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># stress-ng --cpu 28 --vm 12 --hdd 8 --fork 12 --switch 4 --metrics-brief</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_breanking_in_flash_drives"><a class="anchor" href="#_breanking_in_flash_drives"></a>6.2. Breanking-in Flash drives</h3>
<div class="paragraph">
<p>The dd utility is really not useful as a benchmarking tool, but it is an excellent tool to use to break in NVMes before you run a real benchmark. We will run dd in a loop at least 5 times, each time writing to the NVMe until it&#8217;s full. Notice dd is not running against a partition or a file in a filesystem.</p>
</div>
<div class="paragraph">
<p>Example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">dd if=/dev/zero of=/dev/nvme0n1 bs=1M oflag=direct</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_raw_disk_baseline_benchmarking_tests"><a class="anchor" href="#_raw_disk_baseline_benchmarking_tests"></a>6.3. RAW Disk Baseline Benchmarking tests</h3>
<div class="paragraph">
<p>This test helps us compare the performance (IOPS) of each hard drive installed on the servers against a known standard of reference provided by the manufacturer, in this case Intel. So we can be certain that disks are performing as expected before we start adding extra layers of complexity on top of them.</p>
</div>
<div class="paragraph">
<p>This tests will be run locally on each server against each of the 8 disks individually and also against and aggregate of the NVMe disks.</p>
</div>
<div class="paragraph">
<p>With this test we can check each disk is giving similar IOPS, and that each
host is giving and similar IOPS aggregate.</p>
</div>
<div class="paragraph">
<p>The tool that we will use to run these I/O tests against the hardrives is fio, fio spawns a number of threads or processes doing a particular type of I/O action as specified by the user. fio takes a number of global parameters, each inherited by the thread unless otherwise parameters given to them overriding
that setting is given.  The typical use of fio is to write a job file matching the I/O load one wants to simulate.</p>
</div>
<div class="paragraph">
<p><strong>Example:</strong></p>
</div>
<div class="sect3">
<h4 id="_single_disk_iops_fio_test"><a class="anchor" href="#_single_disk_iops_fio_test"></a>6.3.1. Single disk IOPS fio test:</h4>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">[global]
norandommap
refill_buffers
bs=4k
runtime=300
ioengine=libaio
iodepth=32
direct=1
sync=0
buffered=0
randrepeat=0
time_based=1
clocksource=gettimeofday
ramp_time=5
write_bw_log=fio
write_iops_log=fio
write_lat_log=fio
log_avg_msec=1000
write_hist_log=fio
log_hist_msec=10000

[job-/dev/nvme7n1]
filename=/dev/nvmeXn1
rw={randread,randwrite}
size=307200M
numjobs=4</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_full_node_drive_testing"><a class="anchor" href="#_full_node_drive_testing"></a>6.3.2. Full node drive testing:</h4>
<div class="paragraph">
<p>We can run 2 types of workloads:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>small I/O 4k random read/write to get the global IOPS available from each node</p>
</li>
<li>
<p>4MB random read/write to get the maximum throughput we can expect from each node in the cluster.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Using an IO/depth of 16. Looking to get the maximum out of each Ceph OSD node we have configured FIO with 20 concurrent jobs for the 4KB workloads, for the workload with 4MB blocks we only needed to configure 4 concurrent jobs to saturate the disks on the nodes.
Each test was run four times at 60 minutes per run with a two-minute ramp-up time. The average results of those four runs are represented in the figures below.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">[global]
norandommap
refill_buffers
bs=4096k
runtime=300
ioengine=libaio
iodepth=4
direct=1
sync=0
buffered=0
randrepeat=0
time_based=1
clocksource=gettimeofday
ramp_time=100
write_bw_log=fio
write_iops_log=fio
write_lat_log=fio
log_avg_msec=6000
write_hist_log=fio
log_hist_msec=10000

[job-/dev/nvme0n1]
filename=/dev/nvme0n1
rw=randread
size=307200M
numjobs=4

[job-/dev/nvme1n1]
filename=/dev/nvme1n1
rw=randread
size=307200M
numjobs=4

[job-/dev/nvme2n1]
filename=/dev/nvme2n1
rw=randread
size=307200M
numjobs=4

[job-/dev/nvme3n1]
filename=/dev/nvme3n1
rw=randread
size=307200M
numjobs=4</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_network_performance_throughput_and_latency_baseline_tests"><a class="anchor" href="#_network_performance_throughput_and_latency_baseline_tests"></a>6.4. Network Performance: Throughput and Latency baseline tests.</h3>
<div class="paragraph">
<p>Because Ceph is a distributed storage solution it heavily relays on the network, the performance that we are able to get from Ceph is tightly coupled with the performance of the network, this is why we need to run extensive network performance tests so we can be sure that each component in the network is delivering as expected.</p>
</div>
<div class="paragraph">
<p>During the tests we will be measuring 3 major indicators:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Latency</strong> is the time required to transmit a packet across a network</p>
</li>
<li>
<p><strong>Throughput</strong> is defined as the quantity of data being sent/received by unit of time</p>
</li>
<li>
<p><strong>Packet loss</strong> reflects the number of packets lost per 100 packets sent by a host</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>To run these tests we can for example use pbench. Pbench is a Benchmarking and
Performance Analysis Framework that provides a set of pre-packaged scripts to
run some common benchmarks using the collection tools and other facilities that
Pbench provides. Pbench has different modules/add-ons that we can use, for
networking we ca use the pbench_uperf module.</p>
</div>
<div class="paragraph">
<p>uperf represents a new kind of benchmarking tool where instead of running a fixed benchmark or workload, a description (or model) of the workload is provided and the tool generates the load according to the model. By distilling the benchmark or workload into a model, you can now do various things like change the scale of the workload, change different parameters, change protocols, etc and analyse the effect of these changes on your model. Some of the questions you could answer using uperf are bandwidth and latency (unidirectional and bidirectional) with different protocols like TCP, UDP, SCTP or SSL.</p>
</div>
<div class="paragraph">
<p>Two types of tests can be run from each endpoint:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Stream</strong>: Measure bulk data transfer performance, "Throughput in GB/s".</p>
</li>
<li>
<p><strong>RR</strong>: Request/response performance is quoted as "Transactions/s" From a
transaction rate, one can infer one way and round-trip average latency.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>This is an example of iperf3 tests mentioned before from all the Ceph nodes in the cluster and at each of the following endpoints :</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">Ceph nodeX public network  &lt;-&gt; Ceph nodeY public network
Ceph nodeX public network  &lt;-&gt; Ceph nodeY private network
Ceph nodeX public network  &lt;-&gt; Client Nodes
Ceph nodeX public network  &lt;-&gt; Client Nodes</code></pre>
</div>
</div>
</div>
</div>
</div>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <a class="navbar-item" href="https://www.redhat.com/en/technologies/cloud-computing/openshift-container-storage" target="_blank">
      <img src="../../_/img/header_logo.svg" alt="Ceph">
  </a>
</footer>
<script id="site-script" src="../../_/js/site.js" data-ui-root-path="../../_"></script>
<script async src="../../_/js/vendor/highlight.js"></script>
  </body>
</html>
