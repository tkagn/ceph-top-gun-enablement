<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Bluestore :: Ceph Top Gun Enablement</title>
    <link rel="canonical" href="https://likid0.github.io/ceph-top-gun-enablement/training/ceph/ceph_bluestore.html">
    <meta name="generator" content="Antora 3.0.1">
    <link rel="stylesheet" href="../../_/css/site.css">
    <script>var uiRootPath = '../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://docs.ceph.com/en/latest/" target="_blank">
          <img src="../../_/img/header_logo_reverse.svg" height="48px" alt="Ceph">
      </a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Get Help</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://access.redhat.com/documentation/zh-cn/red_hat_ceph_storage/5" target="_blank">Ceph Documentation</a>
            <a class="navbar-item" href="https://bugzilla.redhat.com/describecomponents.cgi?product=Red%20Hat%20OpenShift%20Container%20Storage" target="_blank">Browse Bugs</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Improve Guides</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://github.com/likid0/ceph-top-gun-enablement/issues/new/choose" target="_blank">Open Issue</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">More Infos</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://www.redhat.com/en/blog/channel/red-hat-storage" target="_blank">Our Blog</a>
            <a class="navbar-item" href="https://www.youtube.com/channel/UCoyG8VyvB-XUxQl1mD3T3Gw" target="_blank">Youtube</a>
            <a class="navbar-item" href="https://docs.ceph.com/en/latest/" target="_blank">Ceph Storage Technology</a>
          </div>
        </div>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="training" data-version="master">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Ceph Top-Gun Enablement</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Lab Setup</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="opentlc_lab_env.html">Opentlc Lab Env</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Core Ceph</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_introduction.html">Ceph Introduction</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_architecture.html">Ceph Architecture</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cluster_partitioning.html">Ceph Cluster Partitioning</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_hardware.html">Ceph Hardware Recommendations</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_deployment_intro.html">Ceph Install Methods </a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cephadm_intro.html">Cephadm Orchestrator</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_deploy_basic.html">Deploy Ceph with Cephadm</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cli_intro.html">Ceph CLI basic commands</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_pools.html">Ceph storage pools config</a>
  </li>
  <li class="nav-item is-current-page" data-depth="1">
    <a class="nav-link" href="ceph_bluestore.html">Ceph OSD Bluestore</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_recovery.html">Ceph OSD Failure/Recovery</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph-upgrades_cephadm.html">Upgrade Ceph with Cephadm</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_deployment_challenge.html">Challenge Ceph Deployment</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph RADOS Block Device</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephrbd_intro.html">RADOS Block Device introduction</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">CephFS Shared FileSystem</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephfs_intro.html">CephFS introduction &amp; Deployment</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephfs_advanced.html">CephFS Deep Dive</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cephfs_challenge.html">Challenge Cephfs</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph RadosGW</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_intro.html">RGW Introduction &amp; Deployment</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_arch_deep_dive.html">RGW Deep Dive</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_ha.html">RGW High Availability</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_ssl.html">RGW &amp; Ingress with SSL</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_users_quotas.html">RGW Users &amp; Quotas</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_auth.html">RGW Auth &amp; Authz</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_object_versioning.html">RGW S3 Object Versioning</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_placement_and_storage_classes.html">RGW Placement &amp; Storage Classes</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_life_cycle_management.html">RGW Life Cycle Management</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_bucket_policy.html">RGW S3 Bucket Policy</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_sts_introduction.html">RGW Secure Token Service</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_sts_bucket_role_policy.html">RGW Bucket vs Role Policy</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_multisite.html">RGW Multisite Replication</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_cloudsync.html">RGW Object Cloud Transition</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_presignedurl.html">RGW presigned URL</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_opslog.html">RGW Opslog</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_bucket_notification.html">RGW bucket Notification</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Troubleshooting</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="trouble-shooting-nearfull-osds.html">Troubleshooting nearfull OSDs</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="troubleshooting_bluestore.html">Troubleshooting Bluestore issues</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="trouble-shooting-large-omap-objects.html">Troubleshooting Large Omap Objects</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Benchmarking</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_performance_example.html">Setting the Inital Baseline</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Stretched</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="rhcs-stretched-deploy.html">Ceph Stretch Mode</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Ceph Top-Gun Enablement</span>
    <span class="version">master</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">Ceph Top-Gun Enablement</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">master</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Ceph Top-Gun Enablement</a></li>
    <li>Core Ceph</li>
    <li><a href="ceph_bluestore.html">Ceph OSD Bluestore</a></li>
  </ul>
</nav>
  <div class="edit-this-page"><a href="file:///antora/training/modules/ceph/pages/ceph_bluestore.adoc">Edit this Page</a></div>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Bluestore</h1>
<div class="sect1">
<h2 id="_introduction"><a class="anchor" href="#_introduction"></a>1. Introduction</h2>
<div class="sectionbody">
<div class="paragraph">
<p>BlueStore is the default backend object store for the Ceph OSD daemons. The original object store, FileStore, requires a file system on top of raw block devices. Objects are then written to the file system. Unlike the original FileStore back end, BlueStore stores object directly on the block devices without any file system interface, which improves the performance of the cluster.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Filestore has been Deprecated.
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_bluestore_architecture"><a class="anchor" href="#_bluestore_architecture"></a>2. Bluestore Architecture</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Data is directly written to the raw block device and all metadata operations are managed by RocksDB. The device containing the OSD is divided between RocksDB metadata and the actual user data stored in the cluster.  User data objects are stored as blobs directly on the raw block device, once the data has been written to the block device, RocksDB metadata gets updated with the required details about the new data blobs.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/filestore-vs-bluestore-2.png" alt="Filestore vs Bluestore">
</div>
</div>
<div class="paragraph">
<p>RocksDB is an embedded high-performance key-value store that excels with flash-storage, RocksDB can’t directly write to the raw disk device, it needs and underlying filesystem to store it’s persistent data, this is where BlueFS comes in, BlueFS is a Filesystem developed with the minimal feature set needed by RocksDB to store its sst files.</p>
</div>
<div class="paragraph">
<p>RocksDB uses WAL as a transaction log on persistent storage, unlike Filestore where all the writes went first to the journal, in bluestore we have two different datapaths for writes, one were data is written directly to the block device and the other were we use deferred writes, with deferred writes data gets written to the WAL device and later asynchronously flushed to disk.</p>
</div>
<div class="paragraph">
<p>New Bluestore features:
* it enables compression of data at the lowest level.
<strong> if compression is enabled data blobs allocated on the raw device will be compressed. This means that any data written into RH Ceph Storage, no matter the client used(rbd,rados, etc), can benefit from this feature.
* An additional benefit of BlueStore is that it stores data and meta-data in the cluster with checksums for increased integrity.
</strong> Whenever data is read from persistent storage its checksum is verified</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_bluestore_improvements"><a class="anchor" href="#_bluestore_improvements"></a>3. Bluestore improvements</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Direct management of storage devices
BlueStore consumes raw block devices or partitions. This avoids any intervening layers of abstraction, such as local file systems like XFS, that might limit performance or add complexity.</p>
</li>
<li>
<p>Metadata management with RocksDB
BlueStore uses the RocksDB’ key-value database to manage internal metadata, such as the mapping from object names to block locations on a disk.</p>
</li>
<li>
<p>Full data and metadata checksumming
By default all data and metadata written to BlueStore is protected by one or more checksums. No data or metadata are read from disk or returned to the user without verification.
Efficient copy-on-write</p>
</li>
<li>
<p>The Ceph Block Device and Ceph File System snapshots rely on a copy-on-write clone mechanism that is implemented efficiently in BlueStore. This results in efficient I/O both for regular snapshots and for erasure coded pools which rely on cloning to implement efficient two-phase commits.
No large double-writes</p>
</li>
<li>
<p>BlueStore first writes any new data to unallocated space on a block device, and then commits a RocksDB transaction that updates the object metadata to reference the new region of the disk. Only when the write operation is below a configurable size threshold, it falls back to a write-ahead journaling scheme, similar to how FileStore operates.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_bluestore_layout"><a class="anchor" href="#_bluestore_layout"></a>4. Bluestore Layout</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Bluestore has a number of possible storage layout configurations:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The main device that stores the object data.</p>
</li>
<li>
<p>An optional RocksDB device that stores the metadata</p>
<div class="ulist">
<ul>
<li>
<p>A DB device (identified as block.db in the data directory) can be used for storing BlueStore’s internal metadata. BlueStore (or rather, the embedded RocksDB) will put as much metadata as it can on the DB device to improve performance. If the DB device fills up, metadata will spill back onto the primary device (where it would have been otherwise). Again, it is only helpful to provision a DB device if it is faster than the primary device.</p>
</li>
</ul>
</div>
</li>
<li>
<p>An optional WAL device that stores the transaction logs.</p>
<div class="ulist">
<ul>
<li>
<p>A write-ahead log (WAL) device (identified as block.wal in the data directory) can be used for BlueStore’s internal journal or write-ahead log. It is only useful to use a WAL device if the device is faster than the primary device (e.g., when it is on an SSD and the primary device is an HDD).</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>For example you could configure the main device on the an HDD drive , this is
where the user data will be stored. We then configured the WAL and RocksDB
devices on a faster Flash NVMe drive, having WAL and RocksDB on a high performing drive will give us more IOPS with lower latency.</p>
</div>
<div class="paragraph">
<p>We can check inside an OSD container
<code>/var/lib/ceph/910c8bb8-95bc-11ed-b7d6-2cc26078e4ef/osd.3/</code> dir to get the osd
layout, this OSD has been configured specifiying the 3 devices block,block.db
and block.wal that is why we have 3 different devices for each</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ls -ltra /var/lib/ceph/910c8bb8-95bc-11ed-b7d6-2cc26078e4ef/osd.3/ | grep block
lrwxrwxrwx 1 ceph ceph  111 Jan 16 11:57 block -&gt; /dev/mapper/ceph--f5a5ccbd--5dc2--44d3--9ea1--031153783317-osd--block--2f3561bc--81c5--483e--9cc2--cb1e3bb69dc8
lrwxrwxrwx 1 ceph ceph  108 Jan 16 11:57 block.db -&gt; /dev/mapper/ceph--b6c2103e--6cc0--4040--b117--a5acfa756bc9-osd--db--987930d8--0aaa--4c8f--812b--edc12edb2160
lrwxrwxrwx 1 ceph ceph  109 Jan 16 11:57 block.wal -&gt; /dev/mapper/ceph--d71f301f--cdd5--40f9--b7aa--8ee84c092d81-osd--wal--5de39adc--5a3e--419e--ab3f--3ae239dd2f78</code></pre>
</div>
</div>
<div class="paragraph">
<p>The devices point to LVM device mappers on the Node, so each device is a
dedicated LVM logical volume, each lvol is using a separe VG with a dedicated
drive</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># lvs
  LV                                             VG                                        Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  osd-db-987930d8-0aaa-4c8f-812b-edc12edb2160    ceph-b6c2103e-6cc0-4040-b117-a5acfa756bc9 -wi-ao---- &lt;10.00g
  osd-wal-5de39adc-5a3e-419e-ab3f-3ae239dd2f78   ceph-d71f301f-cdd5-40f9-b7aa-8ee84c092d81 -wi-ao---- &lt;10.00g
  osd-block-2f3561bc-81c5-483e-9cc2-cb1e3bb69dc8 ceph-f5a5ccbd-5dc2-44d3-9ea1-031153783317 -wi-ao---- &lt;10.00g

# pvs
  PV         VG                                        Fmt  Attr PSize   PFree
  /dev/vdc   ceph-f5a5ccbd-5dc2-44d3-9ea1-031153783317 lvm2 a--  &lt;10.00g    0
  /dev/vdd   ceph-d71f301f-cdd5-40f9-b7aa-8ee84c092d81 lvm2 a--  &lt;10.00g    0
  /dev/vde   ceph-b6c2103e-6cc0-4040-b117-a5acfa756bc9 lvm2 a--  &lt;10.00g    0</code></pre>
</div>
</div>
<div class="paragraph">
<p>From inside the OSD container we can also use the <code>ceph-volume</code> command to
check-out the OSD layout details</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph-volume lvm list

====== osd.3 =======

  [db]          /dev/ceph-b6c2103e-6cc0-4040-b117-a5acfa756bc9/osd-db-987930d8-0aaa-4c8f-812b-edc12edb2160

      block device              /dev/ceph-f5a5ccbd-5dc2-44d3-9ea1-031153783317/osd-block-2f3561bc-81c5-483e-9cc2-cb1e3bb69dc8
      block uuid                bP7bb6-RmSZ-2Un8-bj63-yJbR-RK21-KdYcDZ
      cephx lockbox secret
      cluster fsid              910c8bb8-95bc-11ed-b7d6-2cc26078e4ef
      cluster name              ceph
      crush device class
      db device                 /dev/ceph-b6c2103e-6cc0-4040-b117-a5acfa756bc9/osd-db-987930d8-0aaa-4c8f-812b-edc12edb2160
      db uuid                   2Lk2gH-Ud0M-SH2J-GNGE-4DfM-0xIN-CvJ8E0
      encrypted                 0
      osd fsid                  2f3561bc-81c5-483e-9cc2-cb1e3bb69dc8
      osd id                    3
      osdspec affinity          osd_using_paths
      type                      db
      vdo                       0
      wal device                /dev/ceph-d71f301f-cdd5-40f9-b7aa-8ee84c092d81/osd-wal-5de39adc-5a3e-419e-ab3f-3ae239dd2f78
      wal uuid                  VufD8W-g9uX-FIDd-2p6G-90hJ-nBYt-HZG5X7
      devices                   /dev/vde

  [wal]         /dev/ceph-d71f301f-cdd5-40f9-b7aa-8ee84c092d81/osd-wal-5de39adc-5a3e-419e-ab3f-3ae239dd2f78

      block device              /dev/ceph-f5a5ccbd-5dc2-44d3-9ea1-031153783317/osd-block-2f3561bc-81c5-483e-9cc2-cb1e3bb69dc8
      block uuid                bP7bb6-RmSZ-2Un8-bj63-yJbR-RK21-KdYcDZ
      cephx lockbox secret
      cluster fsid              910c8bb8-95bc-11ed-b7d6-2cc26078e4ef
      cluster name              ceph
      crush device class
      encrypted                 0
      osd fsid                  2f3561bc-81c5-483e-9cc2-cb1e3bb69dc8
      osd id                    3
      osdspec affinity          osd_using_paths
      type                      wal
      vdo                       0
      wal device                /dev/ceph-d71f301f-cdd5-40f9-b7aa-8ee84c092d81/osd-wal-5de39adc-5a3e-419e-ab3f-3ae239dd2f78
      wal uuid                  VufD8W-g9uX-FIDd-2p6G-90hJ-nBYt-HZG5X7
      devices                   /dev/vdd

  [block]       /dev/ceph-f5a5ccbd-5dc2-44d3-9ea1-031153783317/osd-block-2f3561bc-81c5-483e-9cc2-cb1e3bb69dc8

      block device              /dev/ceph-f5a5ccbd-5dc2-44d3-9ea1-031153783317/osd-block-2f3561bc-81c5-483e-9cc2-cb1e3bb69dc8
      block uuid                bP7bb6-RmSZ-2Un8-bj63-yJbR-RK21-KdYcDZ
      cephx lockbox secret
      cluster fsid              910c8bb8-95bc-11ed-b7d6-2cc26078e4ef
      cluster name              ceph
      crush device class
      db device                 /dev/ceph-b6c2103e-6cc0-4040-b117-a5acfa756bc9/osd-db-987930d8-0aaa-4c8f-812b-edc12edb2160
      db uuid                   2Lk2gH-Ud0M-SH2J-GNGE-4DfM-0xIN-CvJ8E0
      encrypted                 0
      osd fsid                  2f3561bc-81c5-483e-9cc2-cb1e3bb69dc8
      osd id                    3
      osdspec affinity          osd_using_paths
      type                      block
      vdo                       0
      wal device                /dev/ceph-d71f301f-cdd5-40f9-b7aa-8ee84c092d81/osd-wal-5de39adc-5a3e-419e-ab3f-3ae239dd2f78
      wal uuid                  VufD8W-g9uX-FIDd-2p6G-90hJ-nBYt-HZG5X7
      devices                   /dev/vdc</code></pre>
</div>
</div>
<div class="paragraph">
<p>You can also use the <code>ceph-bluestore-tool</code> to check the labels</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
The OSD process can&#8217;t be running when using the  <code>ceph-bluestore-tool</code>
or the ceph-objectstore-tool tool, check steps on how to start an OSD container
without the OSD process for debugging <a href="troubleshooting_bluestore.html" class="xref page">here</a>.
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph-bluestore-tool show-label --dev /dev/mapper/ceph--f5a5ccbd--5dc2--44d3--9ea1--031153783317-osd--block--2f3561bc--81c5--483e--9cc2--cb1e3bb69dc8
{
    "/dev/mapper/ceph--f5a5ccbd--5dc2--44d3--9ea1--031153783317-osd--block--2f3561bc--81c5--483e--9cc2--cb1e3bb69dc8": {
        "osd_uuid": "2f3561bc-81c5-483e-9cc2-cb1e3bb69dc8",
        "size": 10733223936,
        "btime": "2023-01-16T16:57:43.638530+0000",
        "description": "main",
        "bfm_blocks": "2620416",
        "bfm_blocks_per_key": "128",
        "bfm_bytes_per_block": "4096",
        "bfm_size": "10733223936",
        "bluefs": "1",
        "ceph_fsid": "910c8bb8-95bc-11ed-b7d6-2cc26078e4ef",
        "kv_backend": "rocksdb",
        "magic": "ceph osd volume v026",
        "mkfs_done": "yes",
        "osd_key": "AQAFgsVjR+/KKRAA8QmuVH4WJ9mlDaVcO91xVg==",
        "osdspec_affinity": "osd_using_paths",
        "ready": "ready",
        "require_osd_release": "16",
        "whoami": "3"
    }
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_bluestore_spill_over"><a class="anchor" href="#_bluestore_spill_over"></a>5. Bluestore Spill Over</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Spill Over happens when RocksDB starts using the <code>slow</code> block device affecting
performance.</p>
</div>
<div class="paragraph">
<p>BlueFS wraps RocksDB understanding of filesystem and transforms into structure well suited for block devices.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>db is</strong> allocated from device options.bluestore_block_db_path, (informal block.db)</p>
</li>
<li>
<p><strong>db.slow</strong> is allocated from device options.bluestore_block_path, (informal block)</p>
</li>
<li>
<p><strong>db.wal</strong> is allocated from device options.bluestore_wal_path, (informal block.wal)</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>During processing, RocksDB has a temporary higher demand for space. It asks to create a file on "/db/xxxx" but exhausts space on block.db and starts consuming block space. This redirection of allocation is done internally in BlueFS and RocksDB is unaware that relocation occurred. This means that file with name "/db/xxxxx" is located on slow block device, so RocksDB still thinks it is fast. After the peak is gone, data allocated on block remains there, with a lot of space free on block.db.</p>
</div>
<div class="paragraph">
<p><strong>How to check?</strong></p>
</div>
<div class="paragraph">
<p>If you have spillover you can expect the "slow_bytes" values to be &gt; 0.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph daemon osd.0 perf dump bluefs | grep -E "db_|slow_"
         "db_total_bytes": 21470642176,
         "db_used_bytes": 179699712,
         "slow_total_bytes": 0,
         "slow_used_bytes": 0,</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_bluestore_deployment_strategy"><a class="anchor" href="#_bluestore_deployment_strategy"></a>6. Bluestore deployment Strategy</h2>
<div class="sectionbody">
<div class="paragraph">
<p>It&#8217;s recommended to follow these guidelines:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>If all devices are the same type, for example all rotational drives, and there are no fast devices to use for metadata, it makes sense to specify the block device only and to not separate block.db or block.wal.</p>
</li>
<li>
<p>If you have a mix of fast and slow devices (SSD / NVMe and rotational), it is recommended to place block.db on the faster device while block (data) lives on the slower (spinning drive).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>When using a mixed spinning and solid drive setup it is important to make a large enough block.db logical volume for BlueStore. Generally, block.db should have as large as possible logical volumes.</p>
</div>
<div class="paragraph">
<p>The general recommendation is to have block.db size in between 1% to 4% of block size. For RGW workloads, it is recommended that the block.db size isn’t smaller than 4% of block, because RGW heavily uses it to store metadata (omap keys). For example, if the block size is 1TB, then block.db shouldn’t be less than 40GB. For RBD workloads, 1% to 2% of block size is usually enough.</p>
</div>
<div class="paragraph">
<p>You can check detailed information on the OSD layout with the <code>ceph osd metadata command</code></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph osd metadata osd.0 | grep blue
    "bluefs": "1",
    "bluefs_dedicated_db": "0",
    "bluefs_dedicated_wal": "0",
    "bluefs_single_shared_device": "1",
    "bluestore_bdev_access_mode": "blk",
    "bluestore_bdev_block_size": "4096",
    "bluestore_bdev_dev_node": "/dev/dm-0",
    "bluestore_bdev_devices": "vdb",
    "bluestore_bdev_driver": "KernelDevice",
    "bluestore_bdev_partition_path": "/dev/dm-0",
    "bluestore_bdev_rotational": "1",
    "bluestore_bdev_size": "10733223936",
    "bluestore_bdev_support_discard": "0",
    "bluestore_bdev_type": "hdd",
    "osd_objectstore": "bluestore",</code></pre>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>In older releases, internal level sizes mean that the DB can fully utilize only
specific partition / LV sizes that correspond to sums of L0, L0+L1, L1+L2, etc.
sizes, which with default settings means roughly 3 GB, 30 GB, 300 GB, and so
forth. Most deployments will not substantially benefit from sizing to
accommodate L3 and higher.</p>
</div>
<div class="paragraph">
<p>Improvements in the latest releases beginning with Nautilus 14.2.12 and Octopus 15.2.6 enable better utilization of arbitrary DB device sizes, and the Pacific release brings experimental dynamic level support.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_bluestore_cache"><a class="anchor" href="#_bluestore_cache"></a>7. Bluestore Cache</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The BlueStore cache is a collection of buffers that, depending on configuration, can be populated with data as the OSD daemon does reading from or writing to the disk.</p>
</div>
<div class="sect2">
<h3 id="_bluestore_automatic_cache_sizing"><a class="anchor" href="#_bluestore_automatic_cache_sizing"></a>7.1. Bluestore Automatic Cache Sizing</h3>
<div class="paragraph">
<p>BlueStore can be configured to automatically resize its caches when TCMalloc is configured as the memory allocator and the bluestore_cache_autotune setting is enabled. This option is currently enabled by default. BlueStore will attempt to keep OSD heap memory usage under a designated target size via the osd_memory_target configuration option. This is a best effort algorithm and caches will not shrink smaller than the amount specified by osd_memory_cache_min.</p>
</div>
<div class="paragraph">
<p>Automatic Cache sized works great with most workloads, as such we recommend to
using it.</p>
</div>
</div>
<div class="sect2">
<h3 id="_bluestore_manual_cache_sizing"><a class="anchor" href="#_bluestore_manual_cache_sizing"></a>7.2. Bluestore Manual Cache Sizing</h3>
<div class="paragraph">
<p>When bluestore_cache_autotune is disabled and bluestore_cache_size_ssd parameter is set, BlueStore cache gets subdivided into 3 different caches:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>cache_meta:</strong> used for BlueStore Onode and associated data.</p>
</li>
<li>
<p><strong>cache_kv:</strong> used for RocksDB block cache including indexes/bloom-filters</p>
</li>
<li>
<p><strong>data cache:</strong> used for BlueStore cache for data buffers.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The amount of space that goes to each cache is configurable using ratios, just
an example</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">bluestore_cache_autotune = 0
bluestore_cache_kv_ratio = 0.2
bluestore_cache_meta_ratio = 0.8</code></pre>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
you can check per OSD memory usage details with the following command <code>ceph daemon osd.$OSD_ID dump_mempools"</code>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_the_importance_of_the_bluestore_onode_cache"><a class="anchor" href="#_the_importance_of_the_bluestore_onode_cache"></a>7.3. The importance of the Bluestore Onode Cache.</h3>
<div class="paragraph">
<p>With all NVMe deployments, and specially with RBD workloads the size of the
bluestore cache can have a huge impact on performance. Onode caching in bluestore is hierarchical.  If a onode is not cached, it will be read from the DB disk, populated into the KV cache, and finally populated into the bluestore onode cache, as you can imagine having a direct hit in the Onode cache is much faster than reading from disk or from the KV cache.</p>
</div>
<div class="paragraph">
<p>When all onodes in a data set fit into bluestore&#8217;s block cache, the onodes are never read from disk and thus onodes never have to be populated into the KV cache at all.  This is the optimal scenario for RBD. On the other hand, a worst case scenario is were you end up needing to read onodes from disk, you&#8217;ll end up populating both the rocksdb KV cache and the bluestore onode cache with fresh data and force out older onodes, which may be read back in again from disk later.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_bluestore_database_sharding"><a class="anchor" href="#_bluestore_database_sharding"></a>8. Bluestore Database Sharding</h2>
<div class="sectionbody">
<div class="paragraph">
<p>BlueStore can divide this data into multiple RocksDB column families. When keys have similar access frequency, modification frequency and lifetime, BlueStore benefits from better caching and more precise compaction. This improves performance, and also requires less disk space during compaction, since each column family is smaller and can compact independent of others.</p>
</div>
<div class="paragraph">
<p>OSDs deployed in Pacific or later use RocksDB sharding by default. If Ceph is upgraded to Pacific from a previous version, sharding is off.</p>
</div>
<div class="paragraph">
<p>To check if sharding is enabled on your cluster</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph config get osd.1 bluestore_rocksdb_cf
true
# ceph config get osd.0 bluestore_rocksdb_cfs
m(3) p(3,0-12) O(3,0-13)=block_cache={type=binned_lru} L P</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_minimum_allocation_size"><a class="anchor" href="#_minimum_allocation_size"></a>9. Minimum Allocation Size.</h2>
<div class="sectionbody">
<div class="paragraph">
<p>There is a configured minimum amount of storage that BlueStore will allocate on an OSD. In practice, this is the least amount of capacity that a RADOS object can consume. The value of bluestore_min_alloc_size is derived from the value of bluestore_min_alloc_size_hdd or bluestore_min_alloc_size_ssd depending on the OSD’s rotational attribute.</p>
</div>
<div class="paragraph">
<p>Through the Mimic release, the default values were 64KB and 16KB for rotational (HDD) and non-rotational (SSD) media respectively. Octopus changed the default for SSD (non-rotational) media to 4KB, and Pacific changed the default for HDD (rotational) media to 4KB as well.</p>
</div>
<div class="paragraph">
<p>For example, when an RGW client stores a 1KB S3 object, it is written to a single RADOS object. With the default min_alloc_size value, 4KB of underlying drive space is allocated. This means that roughly (4KB - 1KB) == 3KB is allocated but never used, which corresponds to 300% overhead or 25% efficiency</p>
</div>
<div class="paragraph">
<p>This happens for each replica. So when using the default three copies of data (3R), a 1KB S3 object actually consumes roughly 9KB of storage device capacity. If erasure coding (EC) is used instead of replication, the amplification may be even higher: for a k=4,m=2 pool, our 1KB S3 object will allocate (6 * 4KB) = 24KB of device capacity.</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Note that this BlueStore attribute takes effect only at OSD creation; if changed later, a given OSD’s behavior will not change unless / until it is destroyed and redeployed with the appropriate option value(s). Upgrading to a later Ceph release will not change the value used by OSDs deployed under older releases or with other settings.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_configuring_osds_with_a_separate_device_for_block_and_block_db"><a class="anchor" href="#_configuring_osds_with_a_separate_device_for_block_and_block_db"></a>10. Configuring OSDs with a separate device for Block and Block.DB</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Assuming you have 3 spare/free drives on each of your ceph cluster nodes, like
in this example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph orch device ls | grep Yes
ceph-node01  /dev/vdc  hdd   e17487cd-a216-43db-a  10.7G  Yes        6s ago
ceph-node01  /dev/vdd  hdd   df8a0981-87f2-4fb0-9  10.7G  Yes        6s ago
ceph-node01  /dev/vde  hdd   07526583-dc65-4d07-8  10.7G  Yes        6s ago
ceph-node02  /dev/vdc  hdd   ee4d2082-8ae1-4f6c-8  10.7G  Yes        6s ago
ceph-node02  /dev/vdd  hdd   19dadf78-4369-4bbb-8  10.7G  Yes        6s ago
ceph-node02  /dev/vde  hdd   c3972ab4-7ebb-4f22-b  10.7G  Yes        6s ago
ceph-node03  /dev/vdc  hdd   70ad190e-46a2-44df-8  10.7G  Yes        6s ago
ceph-node03  /dev/vdd  hdd   2ae6c11a-8b4b-480d-8  10.7G  Yes        6s ago
ceph-node03  /dev/vde  hdd   5a556d98-0ec5-4242-b  10.7G  Yes        6s ago</code></pre>
</div>
</div>
<div class="paragraph">
<p>There are many ways in which you can define what drive will be used for each
partition, you can take a look at examples in this <a href="https://docs.ceph.com/en/quincy/cephadm/services/osd/#advanced-osd-service-specifications">link</a></p>
</div>
<div class="paragraph">
<p>In our case first it would not make sense to separate in different drives our db and
wal devices, because all of our disks are the same, but just for the sake of
training let&#8217;s use <code>vde</code> as our flash disk and <code>vdc,vdd</code> as the HDD drives.</p>
</div>
<div class="paragraph">
<p>To make things more complicated we can&#8217;t filter our disk for block.db using the
rotation/non-rotational flags, because all our disks are the same.</p>
</div>
<div class="paragraph">
<p>So our best option is using direct paths filtering option, here is an example
spec to get it working</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># cat osd-path-spec.yaml
---
service_type: osd
service_id: osd_using_paths
placement:
  host_pattern: 'ceph-node0[1-3]'
data_devices:
  paths:
    - /dev/vdc
    - /dev/vdd
db_devices:
  paths:
    - /dev/vde</code></pre>
</div>
</div>
<div class="paragraph">
<p>We can use the <code>dry-run</code> parameter to check what actions will be take by cephadm</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph orch apply -i osd-path-spec.yaml --dry-run
Preview data is being generated.. Please re-run this command in a bit.
# ceph orch apply -i osd-path-spec.yaml --dry-run
################
OSDSPEC PREVIEWS
################
+---------+-----------------+-------------+----------+----------+-----+
|SERVICE  |NAME             |HOST         |DATA      |DB        |WAL  |
+---------+-----------------+-------------+----------+----------+-----+
|osd      |osd_using_paths  |ceph-node01  |/dev/vdc  |/dev/vde  |-    |
|osd      |osd_using_paths  |ceph-node01  |/dev/vdd  |/dev/vde  |-    |
|osd      |osd_using_paths  |ceph-node02  |/dev/vdc  |/dev/vde  |-    |
|osd      |osd_using_paths  |ceph-node02  |/dev/vdd  |/dev/vde  |-    |
|osd      |osd_using_paths  |ceph-node03  |/dev/vdc  |/dev/vde  |-    |
|osd      |osd_using_paths  |ceph-node03  |/dev/vdd  |/dev/vde  |-    |
+---------+-----------------+-------------+----------+----------+-----+</code></pre>
</div>
</div>
<div class="paragraph">
<p>The actions that will take place are looking good, so we can go ahead and apply
the new OSD spec file.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph orch apply -i osd-path-spec.yaml --dry-run
Scheduled osd.osd_using_paths update...</code></pre>
</div>
</div>
<div class="paragraph">
<p>Lets, check if the OSDs are getting configured, we now have no free disks</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph orch device ls | grep Yes
#</code></pre>
</div>
</div>
<div class="paragraph">
<p>With ceph orch, we can see the OSD service created and the daemons running:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph orch ls | grep osd
osd.all-available-devices                     3  34s ago    2d   label:osd
osd.osd_using_paths                           6  34s ago    2m   ceph-node0[1-3]
# ceph orch ps | grep osd
osd.1                            ceph-node02               running (32m)    46s ago   2d     183M    4096M  16.2.10-94.el8cp  34880245f74a  da5d207fe102
osd.10                           ceph-node03               running (2m)     46s ago   2m    98.6M    4096M  16.2.10-94.el8cp  34880245f74a  b316ff73688c
osd.2                            ceph-node03               running (32m)    46s ago   2d     177M    4096M  16.2.10-94.el8cp  34880245f74a  e9323a455610
osd.4                            ceph-node01               running (31m)    46s ago  13h     107M    4096M  16.2.10-94.el8cp  34880245f74a  395c45a7aff0
osd.5                            ceph-node01               running (2m)     46s ago   2m    69.4M    4096M  16.2.10-94.el8cp  34880245f74a  a4827a6ebbc3
osd.6                            ceph-node02               running (2m)     46s ago   2m    90.0M    4096M  16.2.10-94.el8cp  34880245f74a  f45ddb8c8f4c
osd.7                            ceph-node03               running (2m)     46s ago   2m    71.3M    4096M  16.2.10-94.el8cp  34880245f74a  da22f34fecf7
osd.8                            ceph-node01               running (2m)     46s ago   2m    68.6M    4096M  16.2.10-94.el8cp  34880245f74a  fdb82f45e2d7
osd.9                            ceph-node02               running (2m)     46s ago   2m    67.5M    4096M  16.2.10-94.el8cp  34880245f74a  aeed575262aa</code></pre>
</div>
</div>
<div class="paragraph">
<p>With the <code>ceph osd tree</code> command we can check the osds are up+in the cluster</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME                 STATUS  REWEIGHT  PRI-AFF
-1         0.14619  root default
-3         0.14619      datacenter DC1
-2         0.06825          host ceph-node01
 4    hdd  0.00980              osd.4             up   1.00000  1.00000
 5    hdd  0.01459              osd.5             up   1.00000  1.00000
 8    hdd  0.01459              osd.8             up   1.00000  1.00000
-4         0.03897          host ceph-node02
 1    hdd  0.00980              osd.1             up   1.00000  1.00000
 6    hdd  0.01459              osd.6             up   1.00000  1.00000
 9    hdd  0.01459              osd.9             up   1.00000  1.00000
-5         0.03897          host ceph-node03
 2    hdd  0.00980              osd.2             up   1.00000  1.00000
 7    hdd  0.01459              osd.7             up   1.00000  1.00000
10    hdd  0.01459              osd.10            up   1.00000  1.00000</code></pre>
</div>
</div>
<div class="paragraph">
<p>We can check with lvm and <code>ceph-volume lvm list</code> that we are actually using different
devices for each new OSD that we just deployed</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># podman exec -it ceph-910c8bb8-95bc-11ed-b7d6-2cc26078e4ef-osd-8 ceph-volume lvm list
====== osd.4 =======

  [block]       /dev/ceph-45cf10cf-2149-4517-bb8d-25809fb29bbd/osd-block-319ded07-60bd-4715-9f1a-50cdf1872cb6

      block device              /dev/ceph-45cf10cf-2149-4517-bb8d-25809fb29bbd/osd-block-319ded07-60bd-4715-9f1a-50cdf1872cb6
      block uuid                X1ascR-kGpJ-cdAG-VDjm-JYVu-Bghz-6Ie5NK
      cephx lockbox secret
      cluster fsid              910c8bb8-95bc-11ed-b7d6-2cc26078e4ef
      cluster name              ceph
      crush device class
      encrypted                 0
      osd fsid                  319ded07-60bd-4715-9f1a-50cdf1872cb6
      osd id                    4
      osdspec affinity          all-available-devices
      type                      block
      vdo                       0
      devices                   /dev/vdb

====== osd.5 =======

  [block]       /dev/ceph-6b54b025-2567-4197-b4a9-42d85cfb80b9/osd-block-c40763ca-486a-437f-b881-a49d622c69a8

      block device              /dev/ceph-6b54b025-2567-4197-b4a9-42d85cfb80b9/osd-block-c40763ca-486a-437f-b881-a49d622c69a8
      block uuid                1WfFtJ-8lAO-SCWC-Q9JD-FDeD-O4iy-fox5N4
      cephx lockbox secret
      cluster fsid              910c8bb8-95bc-11ed-b7d6-2cc26078e4ef
      cluster name              ceph
      crush device class
      db device                 /dev/ceph-84eee3e3-e8cb-46b1-a980-63c91906cc80/osd-db-e2e64b9d-eeb9-4513-93cf-f6cb0c2e97e6
      db uuid                   4wbS1C-mAy1-E8NZ-X38T-nSe7-RU5F-BNia2w
      encrypted                 0
      osd fsid                  c40763ca-486a-437f-b881-a49d622c69a8
      osd id                    5
      osdspec affinity          osd_using_paths
      type                      block
      vdo                       0
      devices                   /dev/vdc

  [db]          /dev/ceph-84eee3e3-e8cb-46b1-a980-63c91906cc80/osd-db-e2e64b9d-eeb9-4513-93cf-f6cb0c2e97e6

      block device              /dev/ceph-6b54b025-2567-4197-b4a9-42d85cfb80b9/osd-block-c40763ca-486a-437f-b881-a49d622c69a8
      block uuid                1WfFtJ-8lAO-SCWC-Q9JD-FDeD-O4iy-fox5N4
      cephx lockbox secret
      cluster fsid              910c8bb8-95bc-11ed-b7d6-2cc26078e4ef
      cluster name              ceph
      crush device class
      db device                 /dev/ceph-84eee3e3-e8cb-46b1-a980-63c91906cc80/osd-db-e2e64b9d-eeb9-4513-93cf-f6cb0c2e97e6
      db uuid                   4wbS1C-mAy1-E8NZ-X38T-nSe7-RU5F-BNia2w
      encrypted                 0
      osd fsid                  c40763ca-486a-437f-b881-a49d622c69a8
      osd id                    5
      osdspec affinity          osd_using_paths
      type                      db
      vdo                       0
      devices                   /dev/vde

====== osd.8 =======

  [db]          /dev/ceph-84eee3e3-e8cb-46b1-a980-63c91906cc80/osd-db-bcf8b6e9-17ec-48ef-ab77-3d14ab47e51c

      block device              /dev/ceph-930f86f3-2810-44b9-9fee-c550446084cc/osd-block-e0dff673-f64f-4f36-9077-d32ec755a911
      block uuid                FguIB1-Eo48-WNFM-RnPr-05XW-uxKO-bE1BMk
      cephx lockbox secret
      cluster fsid              910c8bb8-95bc-11ed-b7d6-2cc26078e4ef
      cluster name              ceph
      crush device class
      db device                 /dev/ceph-84eee3e3-e8cb-46b1-a980-63c91906cc80/osd-db-bcf8b6e9-17ec-48ef-ab77-3d14ab47e51c
      db uuid                   CXUPGU-gek0-Kncw-bkWe-MqXo-YJcN-qMd812
      encrypted                 0
      osd fsid                  e0dff673-f64f-4f36-9077-d32ec755a911
      osd id                    8
      osdspec affinity          osd_using_paths
      type                      db
      vdo                       0
      devices                   /dev/vde

  [block]       /dev/ceph-930f86f3-2810-44b9-9fee-c550446084cc/osd-block-e0dff673-f64f-4f36-9077-d32ec755a911

      block device              /dev/ceph-930f86f3-2810-44b9-9fee-c550446084cc/osd-block-e0dff673-f64f-4f36-9077-d32ec755a911
      block uuid                FguIB1-Eo48-WNFM-RnPr-05XW-uxKO-bE1BMk
      cephx lockbox secret
      cluster fsid              910c8bb8-95bc-11ed-b7d6-2cc26078e4ef
      cluster name              ceph
      crush device class
      db device                 /dev/ceph-84eee3e3-e8cb-46b1-a980-63c91906cc80/osd-db-bcf8b6e9-17ec-48ef-ab77-3d14ab47e51c
      db uuid                   CXUPGU-gek0-Kncw-bkWe-MqXo-YJcN-qMd812
      encrypted                 0
      osd fsid                  e0dff673-f64f-4f36-9077-d32ec755a911
      osd id                    8
      osdspec affinity          osd_using_paths
      type                      block
      vdo                       0
      devices                   /dev/vdd

# lvs
  LV                                             VG                                        Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  osd-block-319ded07-60bd-4715-9f1a-50cdf1872cb6 ceph-45cf10cf-2149-4517-bb8d-25809fb29bbd -wi-ao---- &lt;10.00g
  osd-block-c40763ca-486a-437f-b881-a49d622c69a8 ceph-6b54b025-2567-4197-b4a9-42d85cfb80b9 -wi-ao---- &lt;10.00g
  osd-db-bcf8b6e9-17ec-48ef-ab77-3d14ab47e51c    ceph-84eee3e3-e8cb-46b1-a980-63c91906cc80 -wi-ao----  &lt;5.00g
  osd-db-e2e64b9d-eeb9-4513-93cf-f6cb0c2e97e6    ceph-84eee3e3-e8cb-46b1-a980-63c91906cc80 -wi-ao----  &lt;5.00g
  osd-block-e0dff673-f64f-4f36-9077-d32ec755a911 ceph-930f86f3-2810-44b9-9fee-c550446084cc -wi-ao---- &lt;10.00g</code></pre>
</div>
</div>
<div class="paragraph">
<p>As a final exercise, how would you remove the osds we just created?. We need
the OSDs out of the ceph-cluster and all the devices in a clean state to be
re-used.</p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>First get rid of the OSDs at the Ceph Level, make sure you don&#8217;t loose data!</p>
</div>
<div class="paragraph">
<p>The Cephadm approach to deleting OSDs</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph orch osd rm 5
Scheduled OSD(s) for removal
# ceph orch osd rm status
OSD  HOST         STATE     PGS  REPLACE  FORCE  ZAP    DRAIN STARTED AT
5    ceph-node01  draining    1  False    False  False  2023-01-19 08:19:27.551848
# ceph orch osd rm status
No OSD remove/replace operations reported
# ceph orch device zap ceph-node01 /dev/vdc --force
zap successful for /dev/vdc on ceph-node01</code></pre>
</div>
</div>
<div class="paragraph">
<p>Example of deleting the osd without the cephadm orchestrator(only use if cephadm leaves
things behind)</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph osd out osd.11
ceph osd down osd.11
ceph osd rm osd.11
ceph osd crush rm osd.11
ceph auth del osd.11</code></pre>
</div>
</div>
</td>
</tr>
</table>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>If the unmanaged flag is unset, cephadm automatically deploys drives that match the OSDSpec. For example, if you use the all-available-devices option when creating OSDs, when you zap a device the cephadm orchestrator automatically creates a new OSD in the device.</p>
</div>
<div class="paragraph">
<p>it’s best to modify the drivegroup spec before removal. Setting the unmanaged:
true to the OSD spec.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <a class="navbar-item" href="https://www.redhat.com/en/technologies/cloud-computing/openshift-container-storage" target="_blank">
      <img src="../../_/img/header_logo.svg" alt="Ceph">
  </a>
</footer>
<script id="site-script" src="../../_/js/site.js" data-ui-root-path="../../_"></script>
<script async src="../../_/js/vendor/highlight.js"></script>
  </body>
</html>
